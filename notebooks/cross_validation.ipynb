{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../modules'))\n",
    "from best_models import models_to_cv\n",
    "from cross_validation import cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train_winsor_1_norm.csv')\n",
    "X_train = train_data.drop(columns='class')\n",
    "y_train = train_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    8659\n",
       "0    4574\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trials = {}\n",
    "model_names = ['KNN','DTR', 'SVM', 'RF', 'XGB', 'LGBM', 'MLP', 'LVQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models, ensembles = models_to_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model KNN\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:150: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 227, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                        CM  f1_score  \\\n",
      "0        KNN  fold_0  0.792614  0.801386   [[359, 99], [172, 694]]  0.836649   \n",
      "1        KNN  fold_1  0.800770  0.800231   [[367, 91], [173, 693]]  0.840000   \n",
      "2        KNN  fold_2  0.789717  0.815242  [[350, 108], [160, 706]]  0.840476   \n",
      "3        KNN  fold_3  0.799398  0.797921   [[366, 91], [175, 691]]  0.838592   \n",
      "4        KNN  fold_4  0.805386  0.796767   [[372, 85], [176, 690]]  0.840951   \n",
      "5        KNN  fold_5  0.787880  0.796767  [[356, 101], [176, 690]]  0.832830   \n",
      "6        KNN  fold_6  0.820429  0.807159   [[381, 76], [167, 699]]  0.851920   \n",
      "7        KNN  fold_7  0.816234  0.814088   [[374, 83], [161, 705]]  0.852479   \n",
      "8        KNN  fold_8  0.813985  0.811778   [[373, 84], [163, 703]]  0.850575   \n",
      "9        KNN  fold_9  0.810416  0.797688   [[377, 81], [175, 690]]  0.843521   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.000000        0.057402    0.204683        0.875202       0.795317   \n",
      "1       0.000000        0.054938    0.199396        0.880051       0.800604   \n",
      "2       0.001005        0.045428    0.202417        0.878347       0.797583   \n",
      "3       0.001533        0.058821    0.201058        0.879051       0.798942   \n",
      "4       0.000505        0.056732    0.197279        0.877627       0.802721   \n",
      "5       0.000000        0.049703    0.209373        0.877563       0.790627   \n",
      "6       0.009996        0.045527    0.183673        0.879424       0.816327   \n",
      "7       0.000000        0.044868    0.184429        0.875748       0.815571   \n",
      "8       0.004472        0.054687    0.186697        0.877201       0.813303   \n",
      "9       0.000000        0.040035    0.193500        0.878088       0.806500   \n",
      "\n",
      "   precision     loacc       auc  \n",
      "0   0.875158  0.267442  0.876635  \n",
      "1   0.883929  0.270755  0.878986  \n",
      "2   0.867322  0.235212  0.872807  \n",
      "3   0.883632  0.286970  0.885263  \n",
      "4   0.890323  0.347153  0.891933  \n",
      "5   0.872314  0.256166  0.874874  \n",
      "6   0.901935  0.310886  0.895051  \n",
      "7   0.894670  0.266255  0.893566  \n",
      "8   0.893266  0.309455  0.894702  \n",
      "9   0.894942  0.276387  0.897168  \n",
      "Model LVQ\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                     CM  f1_score  \\\n",
      "0        LVQ  fold_0  0.515589  0.031178  [[458, 0], [839, 27]]  0.060470   \n",
      "1        LVQ  fold_1  0.515012  0.030023  [[458, 0], [840, 26]]  0.058296   \n",
      "2        LVQ  fold_2  0.517961  0.038106  [[457, 1], [833, 33]]  0.073333   \n",
      "3        LVQ  fold_3  0.523277  0.053118  [[454, 3], [820, 46]]  0.100546   \n",
      "4        LVQ  fold_4  0.519752  0.043880  [[455, 2], [828, 38]]  0.083885   \n",
      "5        LVQ  fold_5  0.514555  0.033487  [[455, 2], [837, 29]]  0.064660   \n",
      "6        LVQ  fold_6  0.506928  0.013857  [[457, 0], [854, 12]]  0.027335   \n",
      "7        LVQ  fold_7  0.512185  0.026559  [[456, 1], [843, 23]]  0.051685   \n",
      "8        LVQ  fold_8  0.519630  0.039261  [[457, 0], [832, 34]]  0.075556   \n",
      "9        LVQ  fold_9  0.518048  0.040462  [[456, 2], [830, 35]]  0.077605   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      73.050814        0.063226    0.633686        0.489342       0.366314   \n",
      "1      31.623950        0.090144    0.634441        0.488700       0.365559   \n",
      "2      26.296785        0.061341    0.629909        0.491547       0.370091   \n",
      "3      27.024206        0.084777    0.622071        0.502392       0.377929   \n",
      "4      42.375026        0.063667    0.627362        0.492120       0.372638   \n",
      "5      27.981052        0.061006    0.634165        0.491230       0.365835   \n",
      "6      27.816826        0.105402    0.645503        0.487821       0.354497   \n",
      "7      29.461844        0.120769    0.637944        0.491024       0.362056   \n",
      "8      28.184279        0.074121    0.628874        0.497580       0.371126   \n",
      "9      31.713835        0.061804    0.628874        0.492414       0.371126   \n",
      "\n",
      "   precision     loacc       auc  \n",
      "0   1.000000  0.096613  0.541969  \n",
      "1   1.000000  0.108545  0.564673  \n",
      "2   0.970588  0.102771  0.530086  \n",
      "3   0.938776  0.078907  0.535274  \n",
      "4   0.950000  0.092379  0.504821  \n",
      "5   0.935484  0.088915  0.528085  \n",
      "6   1.000000  0.053888  0.481977  \n",
      "7   0.958333  0.073518  0.523865  \n",
      "8   1.000000  0.095843  0.530283  \n",
      "9   0.945946  0.087091  0.547853  \n",
      "Model DTR\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                        CM  f1_score  \\\n",
      "0        DTR  fold_0  0.803448  0.838337  [[352, 106], [140, 726]]  0.855124   \n",
      "1        DTR  fold_1  0.795587  0.890300   [[321, 137], [95, 771]]  0.869222   \n",
      "2        DTR  fold_2  0.772890  0.818707  [[333, 125], [157, 709]]  0.834118   \n",
      "3        DTR  fold_3  0.802043  0.794457   [[370, 87], [178, 688]]  0.838513   \n",
      "4        DTR  fold_4  0.797118  0.815242  [[356, 101], [160, 706]]  0.843993   \n",
      "5        DTR  fold_5  0.793105  0.829099  [[346, 111], [148, 718]]  0.847198   \n",
      "6        DTR  fold_6  0.817937  0.795612   [[384, 73], [177, 689]]  0.846437   \n",
      "7        DTR  fold_7  0.792286  0.818707  [[350, 107], [157, 709]]  0.843044   \n",
      "8        DTR  fold_8  0.812891  0.811778   [[372, 85], [163, 703]]  0.850060   \n",
      "9        DTR  fold_9  0.791728  0.793064   [[362, 96], [179, 686]]  0.833030   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.059486        0.000000    0.185801        0.808958       0.814199   \n",
      "1       0.055221        0.000000    0.175227        0.809148       0.824773   \n",
      "2       0.039292        0.000000    0.212991        0.845280       0.787009   \n",
      "3       0.034805        0.001007    0.200302        0.828381       0.799698   \n",
      "4       0.049988        0.000000    0.197279        0.789534       0.802721   \n",
      "5       0.042066        0.000000    0.195767        0.812129       0.804233   \n",
      "6       0.033718        0.000461    0.188964        0.829767       0.811036   \n",
      "7       0.041090        0.000998    0.199546        0.809453       0.800454   \n",
      "8       0.043481        0.001029    0.187453        0.805417       0.812547   \n",
      "9       0.040802        0.000000    0.207861        0.825725       0.792139   \n",
      "\n",
      "   precision     loacc       auc  \n",
      "0   0.872596  0.169267  0.868859  \n",
      "1   0.849119  0.172313  0.868058  \n",
      "2   0.850120  0.123502  0.842896  \n",
      "3   0.887742  0.172644  0.867999  \n",
      "4   0.874845  0.210811  0.868062  \n",
      "5   0.866104  0.192384  0.859841  \n",
      "6   0.904199  0.152896  0.870862  \n",
      "7   0.868873  0.161124  0.863390  \n",
      "8   0.892132  0.238799  0.883369  \n",
      "9   0.877238  0.208542  0.862220  \n",
      "Model SVM\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                        CM  f1_score  \\\n",
      "0        SVM  fold_0  0.709877  0.849885  [[261, 197], [130, 736]]  0.818232   \n",
      "1        SVM  fold_1  0.739270  0.886836   [[271, 187], [98, 768]]  0.843493   \n",
      "2        SVM  fold_2  0.689913  0.958430   [[193, 265], [36, 830]]  0.846507   \n",
      "3        SVM  fold_3  0.728425  0.949192   [[232, 225], [44, 822]]  0.859383   \n",
      "4        SVM  fold_4  0.718161  0.808314  [[287, 170], [166, 700]]  0.806452   \n",
      "5        SVM  fold_5  0.671988  0.945727   [[182, 275], [47, 819]]  0.835714   \n",
      "6        SVM  fold_6  0.780248  0.860277  [[320, 137], [121, 745]]  0.852403   \n",
      "7        SVM  fold_7  0.772861  0.891455   [[299, 158], [94, 772]]  0.859688   \n",
      "8        SVM  fold_8  0.643943  0.856813  [[197, 260], [124, 742]]  0.794433   \n",
      "9        SVM  fold_9  0.723605  0.973410   [[217, 241], [23, 842]]  0.864476   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       4.808489        0.107342    0.246979        0.666622       0.753021   \n",
      "1       4.343540        0.092084    0.215257        0.711192       0.784743   \n",
      "2       4.282743        0.153088    0.227341        0.646402       0.772659   \n",
      "3       4.095264        0.105033    0.203326        0.665993       0.796674   \n",
      "4       4.387470        0.110278    0.253968        0.633149       0.746032   \n",
      "5       4.609495        0.124259    0.243386        0.637750       0.756614   \n",
      "6       4.317971        0.100234    0.195011        0.698964       0.804989   \n",
      "7       4.197393        0.099527    0.190476        0.701405       0.809524   \n",
      "8       4.332109        0.114937    0.290249        0.588856       0.709751   \n",
      "9       4.292301        0.114391    0.199546        0.665145       0.800454   \n",
      "\n",
      "   precision     loacc       auc  \n",
      "0   0.788853  0.269823  0.818965  \n",
      "1   0.804188  0.301386  0.853497  \n",
      "2   0.757991  0.301001  0.840879  \n",
      "3   0.785100  0.242494  0.855496  \n",
      "4   0.804598  0.235951  0.814174  \n",
      "5   0.748629  0.212856  0.804482  \n",
      "6   0.844671  0.272132  0.864460  \n",
      "7   0.830108  0.202848  0.857245  \n",
      "8   0.740519  0.070824  0.696592  \n",
      "9   0.777470  0.369171  0.888884  \n",
      "Model RF\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0         RF  fold_0  0.851977  0.876443  [[379, 79], [107, 759]]  0.890845   \n",
      "1         RF  fold_1  0.860375  0.904157   [[374, 84], [83, 783]]  0.903635   \n",
      "2         RF  fold_2  0.842205  0.898383   [[360, 98], [88, 778]]  0.893226   \n",
      "3         RF  fold_3  0.860723  0.883372  [[383, 74], [101, 765]]  0.897361   \n",
      "4         RF  fold_4  0.848171  0.884527  [[371, 86], [100, 766]]  0.891735   \n",
      "5         RF  fold_5  0.844251  0.881062  [[369, 88], [103, 763]]  0.888759   \n",
      "6         RF  fold_6  0.873639  0.896074   [[389, 68], [90, 776]]  0.907602   \n",
      "7         RF  fold_7  0.859750  0.887991   [[380, 77], [97, 769]]  0.898364   \n",
      "8         RF  fold_8  0.862698  0.896074   [[379, 78], [90, 776]]  0.902326   \n",
      "9         RF  fold_9  0.861668  0.893642   [[380, 78], [92, 773]]  0.900932   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      16.970994        0.074010    0.140483        0.990488       0.859517   \n",
      "1      18.068612        0.067017    0.126133        0.989813       0.873867   \n",
      "2      18.415080        0.098512    0.140483        0.990195       0.859517   \n",
      "3      21.067535        0.093286    0.132275        0.990297       0.867725   \n",
      "4      42.134898        0.173161    0.140590        0.989157       0.859410   \n",
      "5      17.474190        0.081614    0.144369        0.989409       0.855631   \n",
      "6      17.950094        0.075949    0.119426        0.989302       0.880574   \n",
      "7      20.587834        0.180459    0.131519        0.990520       0.868481   \n",
      "8      39.177198        0.084451    0.126984        0.991128       0.873016   \n",
      "9      18.781660        0.112913    0.128496        0.990602       0.871504   \n",
      "\n",
      "   precision     loacc       auc  \n",
      "0   0.905728  0.476905  0.932191  \n",
      "1   0.903114  0.417244  0.929937  \n",
      "2   0.888128  0.434180  0.919938  \n",
      "3   0.911800  0.399538  0.927403  \n",
      "4   0.899061  0.505774  0.932669  \n",
      "5   0.896592  0.491147  0.928732  \n",
      "6   0.919431  0.448807  0.937240  \n",
      "7   0.908983  0.428791  0.934900  \n",
      "8   0.908665  0.522325  0.942554  \n",
      "9   0.908343  0.458960  0.930757  \n",
      "Model XGB\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0        XGB  fold_0  0.861414  0.882217  [[385, 73], [102, 764]]  0.897240   \n",
      "1        XGB  fold_1  0.860827  0.900693   [[376, 82], [86, 780]]  0.902778   \n",
      "2        XGB  fold_2  0.847862  0.885681   [[371, 87], [99, 767]]  0.891860   \n",
      "3        XGB  fold_3  0.859690  0.885681   [[381, 76], [99, 767]]  0.897601   \n",
      "4        XGB  fold_4  0.857897  0.879908  [[382, 75], [104, 762]]  0.894891   \n",
      "5        XGB  fold_5  0.850178  0.877598  [[376, 81], [106, 760]]  0.890451   \n",
      "6        XGB  fold_6  0.873517  0.891455   [[391, 66], [94, 772]]  0.906103   \n",
      "7        XGB  fold_7  0.865495  0.877598  [[390, 67], [106, 760]]  0.897815   \n",
      "8        XGB  fold_8  0.873001  0.892610   [[390, 67], [93, 773]]  0.906213   \n",
      "9        XGB  fold_9  0.867191  0.895954   [[384, 74], [90, 775]]  0.904317   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      30.470511        0.008313    0.132175        0.958581       0.867825   \n",
      "1      21.073300        0.010294    0.126888        0.955879       0.873112   \n",
      "2      19.635098        0.009805    0.140483        0.956992       0.859517   \n",
      "3      20.300445        0.021416    0.132275        0.958628       0.867725   \n",
      "4      20.023418        0.028145    0.135299        0.957839       0.864701   \n",
      "5      21.901587        0.000000    0.141345        0.956962       0.858655   \n",
      "6      23.061731        0.015574    0.120937        0.957745       0.879063   \n",
      "7      23.823815        0.038160    0.130763        0.958045       0.869237   \n",
      "8      24.521904        0.012993    0.120937        0.957051       0.879063   \n",
      "9      29.860890        0.011265    0.123961        0.958848       0.876039   \n",
      "\n",
      "   precision     loacc       auc  \n",
      "0   0.912784  0.428791  0.934783  \n",
      "1   0.904872  0.379138  0.929508  \n",
      "2   0.898126  0.381062  0.922388  \n",
      "3   0.909846  0.416474  0.930155  \n",
      "4   0.910394  0.458045  0.935562  \n",
      "5   0.903686  0.496921  0.934685  \n",
      "6   0.921241  0.440339  0.938789  \n",
      "7   0.918984  0.421093  0.938781  \n",
      "8   0.920238  0.519246  0.945402  \n",
      "9   0.912839  0.382274  0.927130  \n",
      "Model LGBM\n",
      "Fold: 0\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7031\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14824, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525702 -> initscore=0.102897\n",
      "[LightGBM] [Info] Start training from score 0.102897\n",
      "Fold: 1\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14823, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525737 -> initscore=0.103039\n",
      "[LightGBM] [Info] Start training from score 0.103039\n",
      "Fold: 2\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 6995\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14788, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.526981 -> initscore=0.108030\n",
      "[LightGBM] [Info] Start training from score 0.108030\n",
      "Fold: 3\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7048\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14841, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525099 -> initscore=0.100482\n",
      "[LightGBM] [Info] Start training from score 0.100482\n",
      "Fold: 4\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7055\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14848, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524852 -> initscore=0.099489\n",
      "[LightGBM] [Info] Start training from score 0.099489\n",
      "Fold: 5\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7031\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14824, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525702 -> initscore=0.102897\n",
      "[LightGBM] [Info] Start training from score 0.102897\n",
      "Fold: 6\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14862, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524357 -> initscore=0.097507\n",
      "[LightGBM] [Info] Start training from score 0.097507\n",
      "Fold: 7\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7080\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14873, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523970 -> initscore=0.095952\n",
      "[LightGBM] [Info] Start training from score 0.095952\n",
      "Fold: 8\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14878, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523794 -> initscore=0.095246\n",
      "[LightGBM] [Info] Start training from score 0.095246\n",
      "Fold: 9\n",
      "[LightGBM] [Info] Number of positive: 7794, number of negative: 7102\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14896, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523228 -> initscore=0.092978\n",
      "[LightGBM] [Info] Start training from score 0.092978\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0       LGBM  fold_0  0.865393  0.887991   [[386, 72], [97, 769]]  0.900996   \n",
      "1       LGBM  fold_1  0.860438  0.906467   [[373, 85], [81, 785]]  0.904378   \n",
      "2       LGBM  fold_2  0.842205  0.898383   [[360, 98], [88, 778]]  0.893226   \n",
      "3       LGBM  fold_3  0.866133  0.881062  [[389, 68], [103, 763]]  0.899234   \n",
      "4       LGBM  fold_4  0.860723  0.883372  [[383, 74], [101, 765]]  0.897361   \n",
      "5       LGBM  fold_5  0.854736  0.884527  [[377, 80], [100, 766]]  0.894860   \n",
      "6       LGBM  fold_6  0.865281  0.890300   [[384, 73], [95, 771]]  0.901754   \n",
      "7       LGBM  fold_7  0.861058  0.875289  [[387, 70], [108, 758]]  0.894923   \n",
      "8       LGBM  fold_8  0.870082  0.906467   [[381, 76], [81, 785]]  0.909091   \n",
      "9       LGBM  fold_9  0.871686  0.900578   [[386, 72], [86, 779]]  0.907925   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.448829        0.003000    0.127644        0.959997       0.872356   \n",
      "1       0.562813        0.007005    0.125378        0.960062       0.874622   \n",
      "2       0.596211        0.003000    0.140483        0.957330       0.859517   \n",
      "3       0.562890        0.003004    0.129252        0.958830       0.870748   \n",
      "4       1.313647        0.005005    0.132275        0.957233       0.867725   \n",
      "5       0.488029        0.000000    0.136054        0.960402       0.863946   \n",
      "6       0.680049        0.009236    0.126984        0.960772       0.873016   \n",
      "7       0.367891        0.002509    0.134543        0.958852       0.865457   \n",
      "8       0.369129        0.002006    0.118670        0.960344       0.881330   \n",
      "9       0.334795        0.000958    0.119426        0.959788       0.880574   \n",
      "\n",
      "   precision     loacc       auc  \n",
      "0   0.914388  0.416859  0.933754  \n",
      "1   0.902299  0.434950  0.935282  \n",
      "2   0.888128  0.394534  0.921425  \n",
      "3   0.918171  0.414165  0.930481  \n",
      "4   0.911800  0.448037  0.936515  \n",
      "5   0.905437  0.462279  0.932970  \n",
      "6   0.913507  0.451501  0.938531  \n",
      "7   0.915459  0.422248  0.939817  \n",
      "8   0.911731  0.522325  0.943709  \n",
      "9   0.915394  0.345665  0.925408  \n",
      "Model MLP\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n",
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                        CM  f1_score  \\\n",
      "0        MLP  fold_0  0.835569  0.815242   [[392, 66], [160, 706]]  0.862027   \n",
      "1        MLP  fold_1  0.840725  0.904157   [[356, 102], [83, 783]]  0.894346   \n",
      "2        MLP  fold_2  0.834152  0.823326   [[387, 71], [153, 713]]  0.864242   \n",
      "3        MLP  fold_3  0.846990  0.818707   [[400, 57], [157, 709]]  0.868873   \n",
      "4        MLP  fold_4  0.858414  0.878753   [[383, 74], [105, 761]]  0.894768   \n",
      "5        MLP  fold_5  0.826806  0.883372  [[352, 105], [101, 765]]  0.881336   \n",
      "6        MLP  fold_6  0.838237  0.818707   [[392, 65], [157, 709]]  0.864634   \n",
      "7        MLP  fold_7  0.848780  0.866051   [[380, 77], [116, 750]]  0.886001   \n",
      "8        MLP  fold_8  0.872181  0.882217   [[394, 63], [102, 764]]  0.902540   \n",
      "9        MLP  fold_9  0.846124  0.845087   [[388, 70], [134, 731]]  0.877551   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      65.482255        0.015005    0.170695        0.830747       0.829305   \n",
      "1      45.760498        0.016630    0.139728        0.830736       0.860272   \n",
      "2      49.825832        0.024632    0.169184        0.828442       0.830816   \n",
      "3     137.361882        0.023907    0.161754        0.823664       0.838246   \n",
      "4      59.870978        0.000996    0.135299        0.835264       0.864701   \n",
      "5      28.599753        0.009792    0.155707        0.834930       0.844293   \n",
      "6      43.650223        0.027146    0.167800        0.831920       0.832200   \n",
      "7      91.601161        0.021981    0.145881        0.831238       0.854119   \n",
      "8      46.254445        0.018137    0.124717        0.832639       0.875283   \n",
      "9      25.602972        0.001179    0.154195        0.832371       0.845805   \n",
      "\n",
      "   precision     loacc       auc  \n",
      "0   0.914508  0.434180  0.918927  \n",
      "1   0.884746  0.495381  0.925847  \n",
      "2   0.909439  0.410316  0.917565  \n",
      "3   0.925587  0.356043  0.923740  \n",
      "4   0.911377  0.519630  0.934862  \n",
      "5   0.879310  0.467667  0.920366  \n",
      "6   0.916021  0.404157  0.923747  \n",
      "7   0.906892  0.428791  0.926600  \n",
      "8   0.923821  0.530408  0.941455  \n",
      "9   0.912609  0.411561  0.926428  \n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, best_models, use_adasyn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Heterog√™neo\n",
      "Fold: 0\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7031\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14824, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525702 -> initscore=0.102897\n",
      "[LightGBM] [Info] Start training from score 0.102897\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11859, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525677 -> initscore=0.102797\n",
      "[LightGBM] [Info] Start training from score 0.102797\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5625\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11859, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525677 -> initscore=0.102797\n",
      "[LightGBM] [Info] Start training from score 0.102797\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11859, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525677 -> initscore=0.102797\n",
      "[LightGBM] [Info] Start training from score 0.102797\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5624\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11859, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525761 -> initscore=0.103135\n",
      "[LightGBM] [Info] Start training from score 0.103135\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11860, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525717 -> initscore=0.102958\n",
      "[LightGBM] [Info] Start training from score 0.102958\n",
      "Fold: 1\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14823, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525737 -> initscore=0.103039\n",
      "[LightGBM] [Info] Start training from score 0.103039\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5624\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11858, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525721 -> initscore=0.102975\n",
      "[LightGBM] [Info] Start training from score 0.102975\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5624\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000367 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11858, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525721 -> initscore=0.102975\n",
      "[LightGBM] [Info] Start training from score 0.102975\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5624\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11858, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525721 -> initscore=0.102975\n",
      "[LightGBM] [Info] Start training from score 0.102975\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5624\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11859, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525761 -> initscore=0.103135\n",
      "[LightGBM] [Info] Start training from score 0.103135\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5624\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11859, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525761 -> initscore=0.103135\n",
      "[LightGBM] [Info] Start training from score 0.103135\n",
      "Fold: 2\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 6995\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14788, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.526981 -> initscore=0.108030\n",
      "[LightGBM] [Info] Start training from score 0.108030\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5596\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11830, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.526965 -> initscore=0.107966\n",
      "[LightGBM] [Info] Start training from score 0.107966\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5596\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11830, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.526965 -> initscore=0.107966\n",
      "[LightGBM] [Info] Start training from score 0.107966\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5596\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11830, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.526965 -> initscore=0.107966\n",
      "[LightGBM] [Info] Start training from score 0.107966\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5596\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11831, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.527005 -> initscore=0.108127\n",
      "[LightGBM] [Info] Start training from score 0.108127\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5596\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11831, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.527005 -> initscore=0.108127\n",
      "[LightGBM] [Info] Start training from score 0.108127\n",
      "Fold: 3\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7048\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14841, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525099 -> initscore=0.100482\n",
      "[LightGBM] [Info] Start training from score 0.100482\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5638\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11872, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525101 -> initscore=0.100489\n",
      "[LightGBM] [Info] Start training from score 0.100489\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5639\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11873, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525057 -> initscore=0.100311\n",
      "[LightGBM] [Info] Start training from score 0.100311\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5639\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11873, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525057 -> initscore=0.100311\n",
      "[LightGBM] [Info] Start training from score 0.100311\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5638\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11873, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525141 -> initscore=0.100649\n",
      "[LightGBM] [Info] Start training from score 0.100649\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5638\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11873, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525141 -> initscore=0.100649\n",
      "[LightGBM] [Info] Start training from score 0.100649\n",
      "Fold: 4\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7055\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14848, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524852 -> initscore=0.099489\n",
      "[LightGBM] [Info] Start training from score 0.099489\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5644\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.329224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11878, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524836 -> initscore=0.099425\n",
      "[LightGBM] [Info] Start training from score 0.099425\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5644\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11878, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524836 -> initscore=0.099425\n",
      "[LightGBM] [Info] Start training from score 0.099425\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5644\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11878, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524836 -> initscore=0.099425\n",
      "[LightGBM] [Info] Start training from score 0.099425\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5644\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11879, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524876 -> initscore=0.099586\n",
      "[LightGBM] [Info] Start training from score 0.099586\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5644\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11879, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524876 -> initscore=0.099586\n",
      "[LightGBM] [Info] Start training from score 0.099586\n",
      "Fold: 5\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7031\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004288 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14824, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525702 -> initscore=0.102897\n",
      "[LightGBM] [Info] Start training from score 0.102897\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.432428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11859, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525677 -> initscore=0.102797\n",
      "[LightGBM] [Info] Start training from score 0.102797\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11859, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525677 -> initscore=0.102797\n",
      "[LightGBM] [Info] Start training from score 0.102797\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11859, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525677 -> initscore=0.102797\n",
      "[LightGBM] [Info] Start training from score 0.102797\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5624\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11859, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525761 -> initscore=0.103135\n",
      "[LightGBM] [Info] Start training from score 0.103135\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11860, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525717 -> initscore=0.102958\n",
      "[LightGBM] [Info] Start training from score 0.102958\n",
      "Fold: 6\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14862, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524357 -> initscore=0.097507\n",
      "[LightGBM] [Info] Start training from score 0.097507\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5655\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11889, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524350 -> initscore=0.097478\n",
      "[LightGBM] [Info] Start training from score 0.097478\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5655\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11889, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524350 -> initscore=0.097478\n",
      "[LightGBM] [Info] Start training from score 0.097478\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5656\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11890, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524306 -> initscore=0.097301\n",
      "[LightGBM] [Info] Start training from score 0.097301\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5655\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11890, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524390 -> initscore=0.097638\n",
      "[LightGBM] [Info] Start training from score 0.097638\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5655\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11890, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524390 -> initscore=0.097638\n",
      "[LightGBM] [Info] Start training from score 0.097638\n",
      "Fold: 7\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7080\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14873, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523970 -> initscore=0.095952\n",
      "[LightGBM] [Info] Start training from score 0.095952\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5664\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11898, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523954 -> initscore=0.095888\n",
      "[LightGBM] [Info] Start training from score 0.095888\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5664\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11898, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523954 -> initscore=0.095888\n",
      "[LightGBM] [Info] Start training from score 0.095888\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5664\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11898, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523954 -> initscore=0.095888\n",
      "[LightGBM] [Info] Start training from score 0.095888\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5664\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11899, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523994 -> initscore=0.096048\n",
      "[LightGBM] [Info] Start training from score 0.096048\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5664\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11899, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523994 -> initscore=0.096048\n",
      "[LightGBM] [Info] Start training from score 0.096048\n",
      "Fold: 8\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14878, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523794 -> initscore=0.095246\n",
      "[LightGBM] [Info] Start training from score 0.095246\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11902, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523778 -> initscore=0.095182\n",
      "[LightGBM] [Info] Start training from score 0.095182\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11902, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523778 -> initscore=0.095182\n",
      "[LightGBM] [Info] Start training from score 0.095182\n",
      "[LightGBM] [Info] Number of positive: 6234, number of negative: 5668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11902, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523778 -> initscore=0.095182\n",
      "[LightGBM] [Info] Start training from score 0.095182\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11903, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523818 -> initscore=0.095342\n",
      "[LightGBM] [Info] Start training from score 0.095342\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11903, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523818 -> initscore=0.095342\n",
      "[LightGBM] [Info] Start training from score 0.095342\n",
      "Fold: 9\n",
      "[LightGBM] [Info] Number of positive: 7794, number of negative: 7102\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14896, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523228 -> initscore=0.092978\n",
      "[LightGBM] [Info] Start training from score 0.092978\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5681\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11916, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523246 -> initscore=0.093051\n",
      "[LightGBM] [Info] Start training from score 0.093051\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5682\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11917, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523202 -> initscore=0.092875\n",
      "[LightGBM] [Info] Start training from score 0.092875\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5682\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11917, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523202 -> initscore=0.092875\n",
      "[LightGBM] [Info] Start training from score 0.092875\n",
      "[LightGBM] [Info] Number of positive: 6235, number of negative: 5682\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11917, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523202 -> initscore=0.092875\n",
      "[LightGBM] [Info] Start training from score 0.092875\n",
      "[LightGBM] [Info] Number of positive: 6236, number of negative: 5681\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11917, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523286 -> initscore=0.093212\n",
      "[LightGBM] [Info] Start training from score 0.093212\n",
      "\n",
      " Metrics: \n",
      "    model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0  Heterog√™neo  fold_0  0.864175  0.883372  [[387, 71], [101, 765]]  0.898942   \n",
      "1  Heterog√™neo  fold_1  0.864805  0.906467   [[377, 81], [81, 785]]  0.906467   \n",
      "2  Heterog√™neo  fold_2  0.847537  0.893764   [[367, 91], [92, 774]]  0.894281   \n",
      "3  Heterog√™neo  fold_3  0.867106  0.876443  [[392, 65], [107, 759]]  0.898225   \n",
      "4  Heterog√™neo  fold_4  0.863367  0.879908  [[387, 70], [104, 762]]  0.897527   \n",
      "5  Heterog√™neo  fold_5  0.850238  0.879908  [[375, 82], [104, 762]]  0.891228   \n",
      "6  Heterog√™neo  fold_6  0.867348  0.885681   [[388, 69], [99, 767]]  0.901293   \n",
      "7  Heterog√™neo  fold_7  0.865434  0.875289  [[391, 66], [108, 758]]  0.897041   \n",
      "8  Heterog√™neo  fold_8  0.868806  0.899538   [[383, 74], [87, 779]]  0.906341   \n",
      "9  Heterog√™neo  fold_9  0.867126  0.893642   [[385, 73], [92, 773]]  0.903565   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0     184.095204        0.032378    0.129909        0.960200       0.870091   \n",
      "1     298.162383        0.016418    0.122356        0.961209       0.877644   \n",
      "2     274.645435        0.015370    0.138218        0.958750       0.861782   \n",
      "3     230.730672        0.031260    0.130008        0.961391       0.869992   \n",
      "4     235.232438        0.021785    0.131519        0.959591       0.868481   \n",
      "5     223.575740        0.019213    0.140590        0.962426       0.859410   \n",
      "6     100.249672        0.015624    0.126984        0.963329       0.873016   \n",
      "7      98.132000        0.014708    0.131519        0.961339       0.868481   \n",
      "8      98.180575        0.022539    0.121693        0.961890       0.878307   \n",
      "9     110.090298        0.015624    0.124717        0.962607       0.875283   \n",
      "\n",
      "   precision     loacc       auc  \n",
      "0   0.915072  0.456505  0.935902  \n",
      "1   0.906467  0.428791  0.935595  \n",
      "2   0.894798  0.404542  0.923399  \n",
      "3   0.921117  0.412625  0.932101  \n",
      "4   0.915865  0.470747  0.937536  \n",
      "5   0.902844  0.459584  0.933364  \n",
      "6   0.917464  0.459969  0.939406  \n",
      "7   0.919903  0.428791  0.940186  \n",
      "8   0.913247  0.525789  0.944904  \n",
      "9   0.913712  0.373410  0.927430  \n",
      "Model ANNs\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n",
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0       ANNs  fold_0  0.851662  0.864896  [[384, 74], [117, 749]]  0.886915   \n",
      "1       ANNs  fold_1  0.850550  0.904157   [[365, 93], [83, 783]]  0.898967   \n",
      "2       ANNs  fold_2  0.838562  0.864896  [[372, 86], [117, 749]]  0.880658   \n",
      "3       ANNs  fold_3  0.863185  0.872979  [[390, 67], [110, 756]]  0.895204   \n",
      "4       ANNs  fold_4  0.857290  0.856813  [[392, 65], [124, 742]]  0.887029   \n",
      "5       ANNs  fold_5  0.834436  0.861432  [[369, 88], [120, 746]]  0.877647   \n",
      "6       ANNs  fold_6  0.853157  0.866051  [[384, 73], [116, 750]]  0.888099   \n",
      "7       ANNs  fold_7  0.848630  0.839492  [[392, 65], [139, 727]]  0.876960   \n",
      "8       ANNs  fold_8  0.872059  0.877598  [[396, 61], [106, 760]]  0.901008   \n",
      "9       ANNs  fold_9  0.851841  0.873988  [[380, 78], [109, 756]]  0.889935   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      63.723279        0.017010    0.144260        0.834525       0.855740   \n",
      "1      61.813087        0.010071    0.132931        0.827633       0.867069   \n",
      "2      88.298646        0.045686    0.153323        0.834731       0.846677   \n",
      "3      73.768764        0.032140    0.133787        0.830065       0.866213   \n",
      "4     100.952423        0.028126    0.142857        0.834321       0.857143   \n",
      "5      82.022968        0.020670    0.157218        0.832569       0.842782   \n",
      "6      85.084459        0.031641    0.142857        0.838514       0.857143   \n",
      "7      75.838160        0.025088    0.154195        0.830095       0.845805   \n",
      "8      78.988085        0.033714    0.126228        0.829144       0.873772   \n",
      "9      80.797221        0.031003    0.141345        0.836869       0.858655   \n",
      "\n",
      "   precision     loacc       auc  \n",
      "0   0.910085  0.438414  0.925477  \n",
      "1   0.893836  0.477675  0.930572  \n",
      "2   0.897006  0.459969  0.925502  \n",
      "3   0.918591  0.377598  0.925036  \n",
      "4   0.919455  0.534642  0.938493  \n",
      "5   0.894484  0.455735  0.925069  \n",
      "6   0.911300  0.425327  0.925748  \n",
      "7   0.917929  0.433025  0.931317  \n",
      "8   0.925700  0.506928  0.942587  \n",
      "9   0.906475  0.422351  0.923472  \n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, ensembles, use_adasyn=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
