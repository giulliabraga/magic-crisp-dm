{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../modules'))\n",
    "from best_pipelines import models_to_cv\n",
    "from cross_validation import cross_validation\n",
    "from preproc import PhishingDatasetPreproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = PhishingDatasetPreproc()\n",
    "dataset, X, y = prep.basic_operations()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipelines, ensembles = models_to_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model KNN\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0        KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1        KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2        KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3        KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4        KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5        KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6        KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7        KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8        KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9        KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1       0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2       0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3       0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4       0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5       0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6       0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7       0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8       0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9       0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "\n",
      "   precision  \n",
      "0   0.965066  \n",
      "1   0.954644  \n",
      "2   0.959227  \n",
      "3   0.958696  \n",
      "4   0.965142  \n",
      "5   0.952688  \n",
      "6   0.957627  \n",
      "7   0.969631  \n",
      "8   0.969499  \n",
      "9   0.965443  \n",
      "Model LVQ\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5         KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6         KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7         KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8         KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9         KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "10        LVQ  fold_0  0.893494  0.921397  [[322, 50], [36, 422]]  0.907527   \n",
      "11        LVQ  fold_1  0.904005  0.934354  [[325, 47], [30, 427]]  0.917293   \n",
      "12        LVQ  fold_2  0.863777  0.945295  [[291, 81], [25, 432]]  0.890722   \n",
      "13        LVQ  fold_3  0.876718  0.949672  [[299, 73], [23, 434]]  0.900415   \n",
      "14        LVQ  fold_4  0.903411  0.927790  [[327, 45], [33, 424]]  0.915767   \n",
      "15        LVQ  fold_5  0.886198  0.947598  [[306, 65], [24, 434]]  0.907001   \n",
      "16        LVQ  fold_6  0.889729  0.951965  [[307, 64], [22, 436]]  0.910230   \n",
      "17        LVQ  fold_7  0.899863  0.934498  [[321, 50], [30, 428]]  0.914530   \n",
      "18        LVQ  fold_8  0.906158  0.949782  [[320, 51], [23, 435]]  0.921610   \n",
      "19        LVQ  fold_9  0.907505  0.949782  [[321, 50], [23, 435]]  0.922587   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5        0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6        0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7        0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8        0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9        0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "10      18.022866        0.103858    0.103614        0.901622       0.896386   \n",
      "11      18.671879        0.095556    0.092883        0.900563       0.907117   \n",
      "12      18.596905        0.074591    0.127865        0.891450       0.872135   \n",
      "13      31.233515        0.103505    0.115802        0.883543       0.884198   \n",
      "14      20.103187        0.303124    0.094089        0.904985       0.905911   \n",
      "15      65.548773        0.300754    0.107358        0.903109       0.892642   \n",
      "16      66.400597        0.329551    0.103739        0.894532       0.896261   \n",
      "17      67.791979        0.109562    0.096502        0.900965       0.903498   \n",
      "18      40.109460        0.294844    0.089264        0.895202       0.910736   \n",
      "19      62.336055        0.276352    0.088058        0.902975       0.911942   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "5    0.952688  \n",
      "6    0.957627  \n",
      "7    0.969631  \n",
      "8    0.969499  \n",
      "9    0.965443  \n",
      "10   0.894068  \n",
      "11   0.900844  \n",
      "12   0.842105  \n",
      "13   0.856016  \n",
      "14   0.904051  \n",
      "15   0.869739  \n",
      "16   0.872000  \n",
      "17   0.895397  \n",
      "18   0.895062  \n",
      "19   0.896907  \n",
      "Model DTR\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5         KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6         KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7         KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8         KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9         KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "10        LVQ  fold_0  0.893494  0.921397  [[322, 50], [36, 422]]  0.907527   \n",
      "11        LVQ  fold_1  0.904005  0.934354  [[325, 47], [30, 427]]  0.917293   \n",
      "12        LVQ  fold_2  0.863777  0.945295  [[291, 81], [25, 432]]  0.890722   \n",
      "13        LVQ  fold_3  0.876718  0.949672  [[299, 73], [23, 434]]  0.900415   \n",
      "14        LVQ  fold_4  0.903411  0.927790  [[327, 45], [33, 424]]  0.915767   \n",
      "15        LVQ  fold_5  0.886198  0.947598  [[306, 65], [24, 434]]  0.907001   \n",
      "16        LVQ  fold_6  0.889729  0.951965  [[307, 64], [22, 436]]  0.910230   \n",
      "17        LVQ  fold_7  0.899863  0.934498  [[321, 50], [30, 428]]  0.914530   \n",
      "18        LVQ  fold_8  0.906158  0.949782  [[320, 51], [23, 435]]  0.921610   \n",
      "19        LVQ  fold_9  0.907505  0.949782  [[321, 50], [23, 435]]  0.922587   \n",
      "20        DTR  fold_0  0.935490  0.965066  [[337, 35], [16, 442]]  0.945455   \n",
      "21        DTR  fold_1  0.933169  0.949672  [[341, 31], [23, 434]]  0.941432   \n",
      "22        DTR  fold_2  0.926449  0.949672  [[336, 36], [23, 434]]  0.936354   \n",
      "23        DTR  fold_3  0.932763  0.964989  [[335, 37], [16, 441]]  0.943316   \n",
      "24        DTR  fold_4  0.937796  0.956236  [[342, 30], [20, 437]]  0.945887   \n",
      "25        DTR  fold_5  0.926765  0.969432  [[328, 43], [14, 444]]  0.939683   \n",
      "26        DTR  fold_6  0.927789  0.960699  [[332, 39], [18, 440]]  0.939168   \n",
      "27        DTR  fold_7  0.927141  0.943231  [[338, 33], [26, 432]]  0.936078   \n",
      "28        DTR  fold_8  0.936319  0.945415  [[344, 27], [25, 433]]  0.943355   \n",
      "29        DTR  fold_9  0.934783  0.958515  [[338, 33], [19, 439]]  0.944086   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5        0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6        0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7        0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8        0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9        0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "10      18.022866        0.103858    0.103614        0.901622       0.896386   \n",
      "11      18.671879        0.095556    0.092883        0.900563       0.907117   \n",
      "12      18.596905        0.074591    0.127865        0.891450       0.872135   \n",
      "13      31.233515        0.103505    0.115802        0.883543       0.884198   \n",
      "14      20.103187        0.303124    0.094089        0.904985       0.905911   \n",
      "15      65.548773        0.300754    0.107358        0.903109       0.892642   \n",
      "16      66.400597        0.329551    0.103739        0.894532       0.896261   \n",
      "17      67.791979        0.109562    0.096502        0.900965       0.903498   \n",
      "18      40.109460        0.294844    0.089264        0.895202       0.910736   \n",
      "19      62.336055        0.276352    0.088058        0.902975       0.911942   \n",
      "20       1.338725        0.046830    0.061446        0.937408       0.938554   \n",
      "21       1.257717        0.048533    0.065139        0.938354       0.934861   \n",
      "22       1.304446        0.046874    0.071170        0.937148       0.928830   \n",
      "23       1.259512        0.051344    0.063932        0.937818       0.936068   \n",
      "24       1.242452        0.047267    0.060314        0.935674       0.939686   \n",
      "25       1.305387        0.041892    0.068758        0.938622       0.931242   \n",
      "26       1.244323        0.047341    0.068758        0.938354       0.931242   \n",
      "27       1.238985        0.031255    0.071170        0.937282       0.928830   \n",
      "28       1.259360        0.036992    0.062726        0.933798       0.937274   \n",
      "29       1.287800        0.047441    0.062726        0.937416       0.937274   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "5    0.952688  \n",
      "6    0.957627  \n",
      "7    0.969631  \n",
      "8    0.969499  \n",
      "9    0.965443  \n",
      "10   0.894068  \n",
      "11   0.900844  \n",
      "12   0.842105  \n",
      "13   0.856016  \n",
      "14   0.904051  \n",
      "15   0.869739  \n",
      "16   0.872000  \n",
      "17   0.895397  \n",
      "18   0.895062  \n",
      "19   0.896907  \n",
      "20   0.926625  \n",
      "21   0.933333  \n",
      "22   0.923404  \n",
      "23   0.922594  \n",
      "24   0.935760  \n",
      "25   0.911704  \n",
      "26   0.918580  \n",
      "27   0.929032  \n",
      "28   0.941304  \n",
      "29   0.930085  \n",
      "Model SVM\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5         KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6         KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7         KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8         KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9         KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "10        LVQ  fold_0  0.893494  0.921397  [[322, 50], [36, 422]]  0.907527   \n",
      "11        LVQ  fold_1  0.904005  0.934354  [[325, 47], [30, 427]]  0.917293   \n",
      "12        LVQ  fold_2  0.863777  0.945295  [[291, 81], [25, 432]]  0.890722   \n",
      "13        LVQ  fold_3  0.876718  0.949672  [[299, 73], [23, 434]]  0.900415   \n",
      "14        LVQ  fold_4  0.903411  0.927790  [[327, 45], [33, 424]]  0.915767   \n",
      "15        LVQ  fold_5  0.886198  0.947598  [[306, 65], [24, 434]]  0.907001   \n",
      "16        LVQ  fold_6  0.889729  0.951965  [[307, 64], [22, 436]]  0.910230   \n",
      "17        LVQ  fold_7  0.899863  0.934498  [[321, 50], [30, 428]]  0.914530   \n",
      "18        LVQ  fold_8  0.906158  0.949782  [[320, 51], [23, 435]]  0.921610   \n",
      "19        LVQ  fold_9  0.907505  0.949782  [[321, 50], [23, 435]]  0.922587   \n",
      "20        DTR  fold_0  0.935490  0.965066  [[337, 35], [16, 442]]  0.945455   \n",
      "21        DTR  fold_1  0.933169  0.949672  [[341, 31], [23, 434]]  0.941432   \n",
      "22        DTR  fold_2  0.926449  0.949672  [[336, 36], [23, 434]]  0.936354   \n",
      "23        DTR  fold_3  0.932763  0.964989  [[335, 37], [16, 441]]  0.943316   \n",
      "24        DTR  fold_4  0.937796  0.956236  [[342, 30], [20, 437]]  0.945887   \n",
      "25        DTR  fold_5  0.926765  0.969432  [[328, 43], [14, 444]]  0.939683   \n",
      "26        DTR  fold_6  0.927789  0.960699  [[332, 39], [18, 440]]  0.939168   \n",
      "27        DTR  fold_7  0.927141  0.943231  [[338, 33], [26, 432]]  0.936078   \n",
      "28        DTR  fold_8  0.936319  0.945415  [[344, 27], [25, 433]]  0.943355   \n",
      "29        DTR  fold_9  0.934783  0.958515  [[338, 33], [19, 439]]  0.944086   \n",
      "30        SVM  fold_0  0.959431  0.967249  [[354, 18], [15, 443]]  0.964091   \n",
      "31        SVM  fold_1  0.951581  0.964989  [[349, 23], [16, 441]]  0.957655   \n",
      "32        SVM  fold_2  0.951925  0.973742  [[346, 26], [12, 445]]  0.959052   \n",
      "33        SVM  fold_3  0.943516  0.964989  [[343, 29], [16, 441]]  0.951456   \n",
      "34        SVM  fold_4  0.964021  0.973742  [[355, 17], [12, 445]]  0.968444   \n",
      "35        SVM  fold_5  0.944865  0.975983  [[339, 32], [11, 447]]  0.954109   \n",
      "36        SVM  fold_6  0.948328  0.969432  [[344, 27], [14, 444]]  0.955867   \n",
      "37        SVM  fold_7  0.947748  0.962882  [[346, 25], [17, 441]]  0.954545   \n",
      "38        SVM  fold_8  0.956670  0.967249  [[351, 20], [15, 443]]  0.961998   \n",
      "39        SVM  fold_9  0.954043  0.978166  [[345, 26], [10, 448]]  0.961373   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5        0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6        0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7        0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8        0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9        0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "10      18.022866        0.103858    0.103614        0.901622       0.896386   \n",
      "11      18.671879        0.095556    0.092883        0.900563       0.907117   \n",
      "12      18.596905        0.074591    0.127865        0.891450       0.872135   \n",
      "13      31.233515        0.103505    0.115802        0.883543       0.884198   \n",
      "14      20.103187        0.303124    0.094089        0.904985       0.905911   \n",
      "15      65.548773        0.300754    0.107358        0.903109       0.892642   \n",
      "16      66.400597        0.329551    0.103739        0.894532       0.896261   \n",
      "17      67.791979        0.109562    0.096502        0.900965       0.903498   \n",
      "18      40.109460        0.294844    0.089264        0.895202       0.910736   \n",
      "19      62.336055        0.276352    0.088058        0.902975       0.911942   \n",
      "20       1.338725        0.046830    0.061446        0.937408       0.938554   \n",
      "21       1.257717        0.048533    0.065139        0.938354       0.934861   \n",
      "22       1.304446        0.046874    0.071170        0.937148       0.928830   \n",
      "23       1.259512        0.051344    0.063932        0.937818       0.936068   \n",
      "24       1.242452        0.047267    0.060314        0.935674       0.939686   \n",
      "25       1.305387        0.041892    0.068758        0.938622       0.931242   \n",
      "26       1.244323        0.047341    0.068758        0.938354       0.931242   \n",
      "27       1.238985        0.031255    0.071170        0.937282       0.928830   \n",
      "28       1.259360        0.036992    0.062726        0.933798       0.937274   \n",
      "29       1.287800        0.047441    0.062726        0.937416       0.937274   \n",
      "30      10.698344        0.260947    0.039759        0.967029       0.960241   \n",
      "31      10.701469        0.253890    0.047045        0.967167       0.952955   \n",
      "32      10.856422        0.283101    0.045838        0.958054       0.954162   \n",
      "33      10.711261        0.267445    0.054282        0.960332       0.945718   \n",
      "34      10.487811        0.267704    0.034982        0.965827       0.965018   \n",
      "35      10.501683        0.261909    0.051870        0.962477       0.948130   \n",
      "36      10.455854        0.252820    0.049457        0.964755       0.950543   \n",
      "37       8.224359        0.095636    0.050663        0.963147       0.949337   \n",
      "38       4.609559        0.119945    0.042220        0.957652       0.957780   \n",
      "39       4.316487        0.125359    0.043426        0.962611       0.956574   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "5    0.952688  \n",
      "6    0.957627  \n",
      "7    0.969631  \n",
      "8    0.969499  \n",
      "9    0.965443  \n",
      "10   0.894068  \n",
      "11   0.900844  \n",
      "12   0.842105  \n",
      "13   0.856016  \n",
      "14   0.904051  \n",
      "15   0.869739  \n",
      "16   0.872000  \n",
      "17   0.895397  \n",
      "18   0.895062  \n",
      "19   0.896907  \n",
      "20   0.926625  \n",
      "21   0.933333  \n",
      "22   0.923404  \n",
      "23   0.922594  \n",
      "24   0.935760  \n",
      "25   0.911704  \n",
      "26   0.918580  \n",
      "27   0.929032  \n",
      "28   0.941304  \n",
      "29   0.930085  \n",
      "30   0.960954  \n",
      "31   0.950431  \n",
      "32   0.944798  \n",
      "33   0.938298  \n",
      "34   0.963203  \n",
      "35   0.933194  \n",
      "36   0.942675  \n",
      "37   0.946352  \n",
      "38   0.956803  \n",
      "39   0.945148  \n",
      "Model RF\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5         KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6         KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7         KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8         KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9         KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "10        LVQ  fold_0  0.893494  0.921397  [[322, 50], [36, 422]]  0.907527   \n",
      "11        LVQ  fold_1  0.904005  0.934354  [[325, 47], [30, 427]]  0.917293   \n",
      "12        LVQ  fold_2  0.863777  0.945295  [[291, 81], [25, 432]]  0.890722   \n",
      "13        LVQ  fold_3  0.876718  0.949672  [[299, 73], [23, 434]]  0.900415   \n",
      "14        LVQ  fold_4  0.903411  0.927790  [[327, 45], [33, 424]]  0.915767   \n",
      "15        LVQ  fold_5  0.886198  0.947598  [[306, 65], [24, 434]]  0.907001   \n",
      "16        LVQ  fold_6  0.889729  0.951965  [[307, 64], [22, 436]]  0.910230   \n",
      "17        LVQ  fold_7  0.899863  0.934498  [[321, 50], [30, 428]]  0.914530   \n",
      "18        LVQ  fold_8  0.906158  0.949782  [[320, 51], [23, 435]]  0.921610   \n",
      "19        LVQ  fold_9  0.907505  0.949782  [[321, 50], [23, 435]]  0.922587   \n",
      "20        DTR  fold_0  0.935490  0.965066  [[337, 35], [16, 442]]  0.945455   \n",
      "21        DTR  fold_1  0.933169  0.949672  [[341, 31], [23, 434]]  0.941432   \n",
      "22        DTR  fold_2  0.926449  0.949672  [[336, 36], [23, 434]]  0.936354   \n",
      "23        DTR  fold_3  0.932763  0.964989  [[335, 37], [16, 441]]  0.943316   \n",
      "24        DTR  fold_4  0.937796  0.956236  [[342, 30], [20, 437]]  0.945887   \n",
      "25        DTR  fold_5  0.926765  0.969432  [[328, 43], [14, 444]]  0.939683   \n",
      "26        DTR  fold_6  0.927789  0.960699  [[332, 39], [18, 440]]  0.939168   \n",
      "27        DTR  fold_7  0.927141  0.943231  [[338, 33], [26, 432]]  0.936078   \n",
      "28        DTR  fold_8  0.936319  0.945415  [[344, 27], [25, 433]]  0.943355   \n",
      "29        DTR  fold_9  0.934783  0.958515  [[338, 33], [19, 439]]  0.944086   \n",
      "30        SVM  fold_0  0.959431  0.967249  [[354, 18], [15, 443]]  0.964091   \n",
      "31        SVM  fold_1  0.951581  0.964989  [[349, 23], [16, 441]]  0.957655   \n",
      "32        SVM  fold_2  0.951925  0.973742  [[346, 26], [12, 445]]  0.959052   \n",
      "33        SVM  fold_3  0.943516  0.964989  [[343, 29], [16, 441]]  0.951456   \n",
      "34        SVM  fold_4  0.964021  0.973742  [[355, 17], [12, 445]]  0.968444   \n",
      "35        SVM  fold_5  0.944865  0.975983  [[339, 32], [11, 447]]  0.954109   \n",
      "36        SVM  fold_6  0.948328  0.969432  [[344, 27], [14, 444]]  0.955867   \n",
      "37        SVM  fold_7  0.947748  0.962882  [[346, 25], [17, 441]]  0.954545   \n",
      "38        SVM  fold_8  0.956670  0.967249  [[351, 20], [15, 443]]  0.961998   \n",
      "39        SVM  fold_9  0.954043  0.978166  [[345, 26], [10, 448]]  0.961373   \n",
      "40         RF  fold_0  0.962454  0.975983  [[353, 19], [11, 447]]  0.967532   \n",
      "41         RF  fold_1  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "42         RF  fold_2  0.969492  0.984683   [[355, 17], [7, 450]]  0.974026   \n",
      "43         RF  fold_3  0.957301  0.973742  [[350, 22], [12, 445]]  0.963203   \n",
      "44         RF  fold_4  0.963771  0.975930  [[354, 18], [11, 446]]  0.968512   \n",
      "45         RF  fold_5  0.960525  0.980349   [[349, 22], [9, 449]]  0.966631   \n",
      "46         RF  fold_6  0.961873  0.980349   [[350, 21], [9, 449]]  0.967672   \n",
      "47         RF  fold_7  0.963665  0.965066  [[357, 14], [16, 442]]  0.967177   \n",
      "48         RF  fold_8  0.971051  0.982533   [[356, 15], [8, 450]]  0.975081   \n",
      "49         RF  fold_9  0.960525  0.980349   [[349, 22], [9, 449]]  0.966631   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5        0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6        0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7        0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8        0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9        0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "10      18.022866        0.103858    0.103614        0.901622       0.896386   \n",
      "11      18.671879        0.095556    0.092883        0.900563       0.907117   \n",
      "12      18.596905        0.074591    0.127865        0.891450       0.872135   \n",
      "13      31.233515        0.103505    0.115802        0.883543       0.884198   \n",
      "14      20.103187        0.303124    0.094089        0.904985       0.905911   \n",
      "15      65.548773        0.300754    0.107358        0.903109       0.892642   \n",
      "16      66.400597        0.329551    0.103739        0.894532       0.896261   \n",
      "17      67.791979        0.109562    0.096502        0.900965       0.903498   \n",
      "18      40.109460        0.294844    0.089264        0.895202       0.910736   \n",
      "19      62.336055        0.276352    0.088058        0.902975       0.911942   \n",
      "20       1.338725        0.046830    0.061446        0.937408       0.938554   \n",
      "21       1.257717        0.048533    0.065139        0.938354       0.934861   \n",
      "22       1.304446        0.046874    0.071170        0.937148       0.928830   \n",
      "23       1.259512        0.051344    0.063932        0.937818       0.936068   \n",
      "24       1.242452        0.047267    0.060314        0.935674       0.939686   \n",
      "25       1.305387        0.041892    0.068758        0.938622       0.931242   \n",
      "26       1.244323        0.047341    0.068758        0.938354       0.931242   \n",
      "27       1.238985        0.031255    0.071170        0.937282       0.928830   \n",
      "28       1.259360        0.036992    0.062726        0.933798       0.937274   \n",
      "29       1.287800        0.047441    0.062726        0.937416       0.937274   \n",
      "30      10.698344        0.260947    0.039759        0.967029       0.960241   \n",
      "31      10.701469        0.253890    0.047045        0.967167       0.952955   \n",
      "32      10.856422        0.283101    0.045838        0.958054       0.954162   \n",
      "33      10.711261        0.267445    0.054282        0.960332       0.945718   \n",
      "34      10.487811        0.267704    0.034982        0.965827       0.965018   \n",
      "35      10.501683        0.261909    0.051870        0.962477       0.948130   \n",
      "36      10.455854        0.252820    0.049457        0.964755       0.950543   \n",
      "37       8.224359        0.095636    0.050663        0.963147       0.949337   \n",
      "38       4.609559        0.119945    0.042220        0.957652       0.957780   \n",
      "39       4.316487        0.125359    0.043426        0.962611       0.956574   \n",
      "40       3.015382        0.073953    0.036145        0.980164       0.963855   \n",
      "41       2.913314        0.064450    0.036188        0.979764       0.963812   \n",
      "42       2.998008        0.066678    0.028951        0.978826       0.971049   \n",
      "43       3.087422        0.062896    0.041013        0.979898       0.958987   \n",
      "44       2.837864        0.081818    0.034982        0.980166       0.965018   \n",
      "45       2.816962        0.080482    0.037394        0.979496       0.962606   \n",
      "46       2.799946        0.079784    0.036188        0.979630       0.963812   \n",
      "47       2.892308        0.066223    0.036188        0.978826       0.963812   \n",
      "48       3.007813        0.060710    0.027744        0.979228       0.972256   \n",
      "49       2.849108        0.081285    0.037394        0.980032       0.962606   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "5    0.952688  \n",
      "6    0.957627  \n",
      "7    0.969631  \n",
      "8    0.969499  \n",
      "9    0.965443  \n",
      "10   0.894068  \n",
      "11   0.900844  \n",
      "12   0.842105  \n",
      "13   0.856016  \n",
      "14   0.904051  \n",
      "15   0.869739  \n",
      "16   0.872000  \n",
      "17   0.895397  \n",
      "18   0.895062  \n",
      "19   0.896907  \n",
      "20   0.926625  \n",
      "21   0.933333  \n",
      "22   0.923404  \n",
      "23   0.922594  \n",
      "24   0.935760  \n",
      "25   0.911704  \n",
      "26   0.918580  \n",
      "27   0.929032  \n",
      "28   0.941304  \n",
      "29   0.930085  \n",
      "30   0.960954  \n",
      "31   0.950431  \n",
      "32   0.944798  \n",
      "33   0.938298  \n",
      "34   0.963203  \n",
      "35   0.933194  \n",
      "36   0.942675  \n",
      "37   0.946352  \n",
      "38   0.956803  \n",
      "39   0.945148  \n",
      "40   0.959227  \n",
      "41   0.965142  \n",
      "42   0.963597  \n",
      "43   0.952891  \n",
      "44   0.961207  \n",
      "45   0.953291  \n",
      "46   0.955319  \n",
      "47   0.969298  \n",
      "48   0.967742  \n",
      "49   0.953291  \n",
      "Model XGB\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5         KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6         KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7         KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8         KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9         KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "10        LVQ  fold_0  0.893494  0.921397  [[322, 50], [36, 422]]  0.907527   \n",
      "11        LVQ  fold_1  0.904005  0.934354  [[325, 47], [30, 427]]  0.917293   \n",
      "12        LVQ  fold_2  0.863777  0.945295  [[291, 81], [25, 432]]  0.890722   \n",
      "13        LVQ  fold_3  0.876718  0.949672  [[299, 73], [23, 434]]  0.900415   \n",
      "14        LVQ  fold_4  0.903411  0.927790  [[327, 45], [33, 424]]  0.915767   \n",
      "15        LVQ  fold_5  0.886198  0.947598  [[306, 65], [24, 434]]  0.907001   \n",
      "16        LVQ  fold_6  0.889729  0.951965  [[307, 64], [22, 436]]  0.910230   \n",
      "17        LVQ  fold_7  0.899863  0.934498  [[321, 50], [30, 428]]  0.914530   \n",
      "18        LVQ  fold_8  0.906158  0.949782  [[320, 51], [23, 435]]  0.921610   \n",
      "19        LVQ  fold_9  0.907505  0.949782  [[321, 50], [23, 435]]  0.922587   \n",
      "20        DTR  fold_0  0.935490  0.965066  [[337, 35], [16, 442]]  0.945455   \n",
      "21        DTR  fold_1  0.933169  0.949672  [[341, 31], [23, 434]]  0.941432   \n",
      "22        DTR  fold_2  0.926449  0.949672  [[336, 36], [23, 434]]  0.936354   \n",
      "23        DTR  fold_3  0.932763  0.964989  [[335, 37], [16, 441]]  0.943316   \n",
      "24        DTR  fold_4  0.937796  0.956236  [[342, 30], [20, 437]]  0.945887   \n",
      "25        DTR  fold_5  0.926765  0.969432  [[328, 43], [14, 444]]  0.939683   \n",
      "26        DTR  fold_6  0.927789  0.960699  [[332, 39], [18, 440]]  0.939168   \n",
      "27        DTR  fold_7  0.927141  0.943231  [[338, 33], [26, 432]]  0.936078   \n",
      "28        DTR  fold_8  0.936319  0.945415  [[344, 27], [25, 433]]  0.943355   \n",
      "29        DTR  fold_9  0.934783  0.958515  [[338, 33], [19, 439]]  0.944086   \n",
      "30        SVM  fold_0  0.959431  0.967249  [[354, 18], [15, 443]]  0.964091   \n",
      "31        SVM  fold_1  0.951581  0.964989  [[349, 23], [16, 441]]  0.957655   \n",
      "32        SVM  fold_2  0.951925  0.973742  [[346, 26], [12, 445]]  0.959052   \n",
      "33        SVM  fold_3  0.943516  0.964989  [[343, 29], [16, 441]]  0.951456   \n",
      "34        SVM  fold_4  0.964021  0.973742  [[355, 17], [12, 445]]  0.968444   \n",
      "35        SVM  fold_5  0.944865  0.975983  [[339, 32], [11, 447]]  0.954109   \n",
      "36        SVM  fold_6  0.948328  0.969432  [[344, 27], [14, 444]]  0.955867   \n",
      "37        SVM  fold_7  0.947748  0.962882  [[346, 25], [17, 441]]  0.954545   \n",
      "38        SVM  fold_8  0.956670  0.967249  [[351, 20], [15, 443]]  0.961998   \n",
      "39        SVM  fold_9  0.954043  0.978166  [[345, 26], [10, 448]]  0.961373   \n",
      "40         RF  fold_0  0.962454  0.975983  [[353, 19], [11, 447]]  0.967532   \n",
      "41         RF  fold_1  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "42         RF  fold_2  0.969492  0.984683   [[355, 17], [7, 450]]  0.974026   \n",
      "43         RF  fold_3  0.957301  0.973742  [[350, 22], [12, 445]]  0.963203   \n",
      "44         RF  fold_4  0.963771  0.975930  [[354, 18], [11, 446]]  0.968512   \n",
      "45         RF  fold_5  0.960525  0.980349   [[349, 22], [9, 449]]  0.966631   \n",
      "46         RF  fold_6  0.961873  0.980349   [[350, 21], [9, 449]]  0.967672   \n",
      "47         RF  fold_7  0.963665  0.965066  [[357, 14], [16, 442]]  0.967177   \n",
      "48         RF  fold_8  0.971051  0.982533   [[356, 15], [8, 450]]  0.975081   \n",
      "49         RF  fold_9  0.960525  0.980349   [[349, 22], [9, 449]]  0.966631   \n",
      "50        XGB  fold_0  0.970518  0.975983  [[359, 13], [11, 447]]  0.973856   \n",
      "51        XGB  fold_1  0.968054  0.973742  [[358, 14], [12, 445]]  0.971616   \n",
      "52        XGB  fold_2  0.973524  0.984683   [[358, 14], [7, 450]]  0.977199   \n",
      "53        XGB  fold_3  0.959645  0.964989  [[355, 17], [16, 441]]  0.963934   \n",
      "54        XGB  fold_4  0.972930  0.978118  [[360, 12], [10, 447]]  0.975983   \n",
      "55        XGB  fold_5  0.965404  0.984716   [[351, 20], [7, 451]]  0.970936   \n",
      "56        XGB  fold_6  0.971374  0.991266   [[353, 18], [4, 454]]  0.976344   \n",
      "57        XGB  fold_7  0.970983  0.971616  [[360, 11], [13, 445]]  0.973742   \n",
      "58        XGB  fold_8  0.974002  0.980349   [[359, 12], [9, 449]]  0.977149   \n",
      "59        XGB  fold_9  0.969447  0.984716   [[354, 17], [7, 451]]  0.974082   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5        0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6        0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7        0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8        0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9        0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "10      18.022866        0.103858    0.103614        0.901622       0.896386   \n",
      "11      18.671879        0.095556    0.092883        0.900563       0.907117   \n",
      "12      18.596905        0.074591    0.127865        0.891450       0.872135   \n",
      "13      31.233515        0.103505    0.115802        0.883543       0.884198   \n",
      "14      20.103187        0.303124    0.094089        0.904985       0.905911   \n",
      "15      65.548773        0.300754    0.107358        0.903109       0.892642   \n",
      "16      66.400597        0.329551    0.103739        0.894532       0.896261   \n",
      "17      67.791979        0.109562    0.096502        0.900965       0.903498   \n",
      "18      40.109460        0.294844    0.089264        0.895202       0.910736   \n",
      "19      62.336055        0.276352    0.088058        0.902975       0.911942   \n",
      "20       1.338725        0.046830    0.061446        0.937408       0.938554   \n",
      "21       1.257717        0.048533    0.065139        0.938354       0.934861   \n",
      "22       1.304446        0.046874    0.071170        0.937148       0.928830   \n",
      "23       1.259512        0.051344    0.063932        0.937818       0.936068   \n",
      "24       1.242452        0.047267    0.060314        0.935674       0.939686   \n",
      "25       1.305387        0.041892    0.068758        0.938622       0.931242   \n",
      "26       1.244323        0.047341    0.068758        0.938354       0.931242   \n",
      "27       1.238985        0.031255    0.071170        0.937282       0.928830   \n",
      "28       1.259360        0.036992    0.062726        0.933798       0.937274   \n",
      "29       1.287800        0.047441    0.062726        0.937416       0.937274   \n",
      "30      10.698344        0.260947    0.039759        0.967029       0.960241   \n",
      "31      10.701469        0.253890    0.047045        0.967167       0.952955   \n",
      "32      10.856422        0.283101    0.045838        0.958054       0.954162   \n",
      "33      10.711261        0.267445    0.054282        0.960332       0.945718   \n",
      "34      10.487811        0.267704    0.034982        0.965827       0.965018   \n",
      "35      10.501683        0.261909    0.051870        0.962477       0.948130   \n",
      "36      10.455854        0.252820    0.049457        0.964755       0.950543   \n",
      "37       8.224359        0.095636    0.050663        0.963147       0.949337   \n",
      "38       4.609559        0.119945    0.042220        0.957652       0.957780   \n",
      "39       4.316487        0.125359    0.043426        0.962611       0.956574   \n",
      "40       3.015382        0.073953    0.036145        0.980164       0.963855   \n",
      "41       2.913314        0.064450    0.036188        0.979764       0.963812   \n",
      "42       2.998008        0.066678    0.028951        0.978826       0.971049   \n",
      "43       3.087422        0.062896    0.041013        0.979898       0.958987   \n",
      "44       2.837864        0.081818    0.034982        0.980166       0.965018   \n",
      "45       2.816962        0.080482    0.037394        0.979496       0.962606   \n",
      "46       2.799946        0.079784    0.036188        0.979630       0.963812   \n",
      "47       2.892308        0.066223    0.036188        0.978826       0.963812   \n",
      "48       3.007813        0.060710    0.027744        0.979228       0.972256   \n",
      "49       2.849108        0.081285    0.037394        0.980032       0.962606   \n",
      "50       3.327116        0.010613    0.028916        0.989546       0.971084   \n",
      "51       3.408116        0.014066    0.031363        0.989413       0.968637   \n",
      "52       3.245558        0.000000    0.025332        0.989145       0.974668   \n",
      "53       3.108377        0.013917    0.039807        0.989681       0.960193   \n",
      "54       3.134388        0.000000    0.026538        0.988207       0.973462   \n",
      "55       3.433359        0.016898    0.032569        0.989145       0.967431   \n",
      "56       3.474054        0.009282    0.026538        0.990083       0.973462   \n",
      "57       3.644018        0.007785    0.028951        0.988609       0.971049   \n",
      "58       3.379151        0.006769    0.025332        0.988877       0.974668   \n",
      "59       3.366447        0.008769    0.028951        0.989413       0.971049   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "5    0.952688  \n",
      "6    0.957627  \n",
      "7    0.969631  \n",
      "8    0.969499  \n",
      "9    0.965443  \n",
      "10   0.894068  \n",
      "11   0.900844  \n",
      "12   0.842105  \n",
      "13   0.856016  \n",
      "14   0.904051  \n",
      "15   0.869739  \n",
      "16   0.872000  \n",
      "17   0.895397  \n",
      "18   0.895062  \n",
      "19   0.896907  \n",
      "20   0.926625  \n",
      "21   0.933333  \n",
      "22   0.923404  \n",
      "23   0.922594  \n",
      "24   0.935760  \n",
      "25   0.911704  \n",
      "26   0.918580  \n",
      "27   0.929032  \n",
      "28   0.941304  \n",
      "29   0.930085  \n",
      "30   0.960954  \n",
      "31   0.950431  \n",
      "32   0.944798  \n",
      "33   0.938298  \n",
      "34   0.963203  \n",
      "35   0.933194  \n",
      "36   0.942675  \n",
      "37   0.946352  \n",
      "38   0.956803  \n",
      "39   0.945148  \n",
      "40   0.959227  \n",
      "41   0.965142  \n",
      "42   0.963597  \n",
      "43   0.952891  \n",
      "44   0.961207  \n",
      "45   0.953291  \n",
      "46   0.955319  \n",
      "47   0.969298  \n",
      "48   0.967742  \n",
      "49   0.953291  \n",
      "50   0.971739  \n",
      "51   0.969499  \n",
      "52   0.969828  \n",
      "53   0.962882  \n",
      "54   0.973856  \n",
      "55   0.957537  \n",
      "56   0.961864  \n",
      "57   0.975877  \n",
      "58   0.973970  \n",
      "59   0.963675  \n",
      "Model LGBM\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:150: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] O sistema no pode encontrar o arquivo especificado\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 227, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3343\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7461, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551937 -> initscore=0.208499\n",
      "[LightGBM] [Info] Start training from score 0.208499\n",
      "Fold: 1\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 3343\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551997 -> initscore=0.208742\n",
      "[LightGBM] [Info] Start training from score 0.208742\n",
      "Fold: 2\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 3343\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551997 -> initscore=0.208742\n",
      "[LightGBM] [Info] Start training from score 0.208742\n",
      "Fold: 3\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 3343\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551997 -> initscore=0.208742\n",
      "[LightGBM] [Info] Start training from score 0.208742\n",
      "Fold: 4\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 3343\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551997 -> initscore=0.208742\n",
      "[LightGBM] [Info] Start training from score 0.208742\n",
      "Fold: 5\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551863 -> initscore=0.208200\n",
      "[LightGBM] [Info] Start training from score 0.208200\n",
      "Fold: 6\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551863 -> initscore=0.208200\n",
      "[LightGBM] [Info] Start training from score 0.208200\n",
      "Fold: 7\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551863 -> initscore=0.208200\n",
      "[LightGBM] [Info] Start training from score 0.208200\n",
      "Fold: 8\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551863 -> initscore=0.208200\n",
      "[LightGBM] [Info] Start training from score 0.208200\n",
      "Fold: 9\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551863 -> initscore=0.208200\n",
      "[LightGBM] [Info] Start training from score 0.208200\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "..        ...     ...       ...       ...                     ...       ...   \n",
      "65       LGBM  fold_5  0.970539  0.986900   [[354, 17], [6, 452]]  0.975189   \n",
      "66       LGBM  fold_6  0.974326  0.989083   [[356, 15], [5, 453]]  0.978402   \n",
      "67       LGBM  fold_7  0.971819  0.975983  [[359, 12], [11, 447]]  0.974918   \n",
      "68       LGBM  fold_8  0.973166  0.975983  [[360, 11], [11, 447]]  0.975983   \n",
      "69       LGBM  fold_9  0.967264  0.980349   [[354, 17], [9, 449]]  0.971861   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "..            ...             ...         ...             ...            ...   \n",
      "65       1.617600        0.003036    0.027744        0.990485       0.972256   \n",
      "66       1.725353        0.000000    0.024125        0.990753       0.975875   \n",
      "67       1.572685        0.000000    0.027744        0.989547       0.972256   \n",
      "68       1.718928        0.003969    0.026538        0.989547       0.973462   \n",
      "69       1.664020        0.009906    0.031363        0.990217       0.968637   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "..        ...  \n",
      "65   0.963753  \n",
      "66   0.967949  \n",
      "67   0.973856  \n",
      "68   0.975983  \n",
      "69   0.963519  \n",
      "\n",
      "[70 rows x 12 columns]\n",
      "Model MLP\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "..        ...     ...       ...       ...                     ...       ...   \n",
      "75        MLP  fold_5  0.958854  0.971616  [[351, 20], [13, 445]]  0.964247   \n",
      "76        MLP  fold_6  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "77        MLP  fold_7  0.972910  0.978166  [[359, 12], [10, 448]]  0.976035   \n",
      "78        MLP  fold_8  0.976954  0.978166   [[362, 9], [10, 448]]  0.979235   \n",
      "79        MLP  fold_9  0.962129  0.978166  [[351, 20], [10, 448]]  0.967603   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "..            ...             ...         ...             ...            ...   \n",
      "75      38.277667        0.000000    0.039807        0.990753       0.960193   \n",
      "76      33.282256        0.004445    0.030157        0.991021       0.969843   \n",
      "77      31.769971        0.016021    0.026538        0.989681       0.973462   \n",
      "78      39.856768        0.008228    0.022919        0.989949       0.977081   \n",
      "79      31.650758        0.007514    0.036188        0.989815       0.963812   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "..        ...  \n",
      "75   0.956989  \n",
      "76   0.969631  \n",
      "77   0.973913  \n",
      "78   0.980306  \n",
      "79   0.957265  \n",
      "\n",
      "[80 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, best_pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Heterogneo\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "    model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0  Heterogneo  fold_0  0.974885  0.984716   [[359, 13], [7, 451]]  0.978308   \n",
      "1  Heterogneo  fold_1  0.970242  0.978118  [[358, 14], [10, 447]]  0.973856   \n",
      "2  Heterogneo  fold_2  0.971680  0.989059   [[355, 17], [5, 452]]  0.976242   \n",
      "3  Heterogneo  fold_3  0.962927  0.971554  [[355, 17], [13, 444]]  0.967320   \n",
      "4  Heterogneo  fold_4  0.970742  0.973742  [[360, 12], [12, 445]]  0.973742   \n",
      "5  Heterogneo  fold_5  0.966752  0.984716   [[352, 19], [7, 451]]  0.971983   \n",
      "6  Heterogneo  fold_6  0.970539  0.986900   [[354, 17], [6, 452]]  0.975189   \n",
      "7  Heterogneo  fold_7  0.982412  0.989083    [[362, 9], [5, 453]]  0.984783   \n",
      "8  Heterogneo  fold_8  0.974258  0.978166  [[360, 11], [10, 448]]  0.977099   \n",
      "9  Heterogneo  fold_9  0.969447  0.984716   [[354, 17], [7, 451]]  0.974082   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       7.060600        0.344222    0.024096        0.990752       0.975904   \n",
      "1       7.649313        0.342451    0.028951        0.989949       0.971049   \n",
      "2       6.496947        0.332809    0.026538        0.989949       0.973462   \n",
      "3       6.315637        0.271440    0.036188        0.990485       0.963812   \n",
      "4       7.024124        0.598225    0.028951        0.989815       0.971049   \n",
      "5      12.987120        0.582011    0.031363        0.991021       0.968637   \n",
      "6      13.307468        0.424559    0.027744        0.990485       0.972256   \n",
      "7       6.128471        0.296288    0.016888        0.989279       0.983112   \n",
      "8       6.333743        0.302859    0.025332        0.990083       0.974668   \n",
      "9       5.934883        0.264646    0.028951        0.990217       0.971049   \n",
      "\n",
      "   precision  \n",
      "0   0.971983  \n",
      "1   0.969631  \n",
      "2   0.963753  \n",
      "3   0.963124  \n",
      "4   0.973742  \n",
      "5   0.959574  \n",
      "6   0.963753  \n",
      "7   0.980519  \n",
      "8   0.976035  \n",
      "9   0.963675  \n",
      "Model ANNs\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "     model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0   Heterogneo  fold_0  0.974885  0.984716   [[359, 13], [7, 451]]  0.978308   \n",
      "1   Heterogneo  fold_1  0.970242  0.978118  [[358, 14], [10, 447]]  0.973856   \n",
      "2   Heterogneo  fold_2  0.971680  0.989059   [[355, 17], [5, 452]]  0.976242   \n",
      "3   Heterogneo  fold_3  0.962927  0.971554  [[355, 17], [13, 444]]  0.967320   \n",
      "4   Heterogneo  fold_4  0.970742  0.973742  [[360, 12], [12, 445]]  0.973742   \n",
      "5   Heterogneo  fold_5  0.966752  0.984716   [[352, 19], [7, 451]]  0.971983   \n",
      "6   Heterogneo  fold_6  0.970539  0.986900   [[354, 17], [6, 452]]  0.975189   \n",
      "7   Heterogneo  fold_7  0.982412  0.989083    [[362, 9], [5, 453]]  0.984783   \n",
      "8   Heterogneo  fold_8  0.974258  0.978166  [[360, 11], [10, 448]]  0.977099   \n",
      "9   Heterogneo  fold_9  0.969447  0.984716   [[354, 17], [7, 451]]  0.974082   \n",
      "10         ANNs  fold_0  0.966404  0.965066  [[360, 12], [16, 442]]  0.969298   \n",
      "11         ANNs  fold_1  0.968898  0.978118  [[357, 15], [10, 447]]  0.972797   \n",
      "12         ANNs  fold_2  0.974368  0.989059   [[357, 15], [5, 452]]  0.978355   \n",
      "13         ANNs  fold_3  0.965271  0.962801  [[360, 12], [17, 440]]  0.968097   \n",
      "14         ANNs  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "15         ANNs  fold_5  0.962964  0.982533   [[350, 21], [8, 450]]  0.968784   \n",
      "16         ANNs  fold_6  0.967008  0.982533   [[353, 18], [8, 450]]  0.971922   \n",
      "17         ANNs  fold_7  0.979069  0.971616   [[366, 5], [13, 445]]  0.980176   \n",
      "18         ANNs  fold_8  0.972654  0.980349   [[358, 13], [9, 449]]  0.976087   \n",
      "19         ANNs  fold_9  0.973490  0.984716   [[357, 14], [7, 451]]  0.977248   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        7.060600        0.344222    0.024096        0.990752       0.975904   \n",
      "1        7.649313        0.342451    0.028951        0.989949       0.971049   \n",
      "2        6.496947        0.332809    0.026538        0.989949       0.973462   \n",
      "3        6.315637        0.271440    0.036188        0.990485       0.963812   \n",
      "4        7.024124        0.598225    0.028951        0.989815       0.971049   \n",
      "5       12.987120        0.582011    0.031363        0.991021       0.968637   \n",
      "6       13.307468        0.424559    0.027744        0.990485       0.972256   \n",
      "7        6.128471        0.296288    0.016888        0.989279       0.983112   \n",
      "8        6.333743        0.302859    0.025332        0.990083       0.974668   \n",
      "9        5.934883        0.264646    0.028951        0.990217       0.971049   \n",
      "10     104.134757        0.015638    0.033735        0.990752       0.966265   \n",
      "11     103.611469        0.021456    0.030157        0.990753       0.969843   \n",
      "12      81.039746        0.015717    0.024125        0.990485       0.975875   \n",
      "13      83.624569        0.015656    0.034982        0.990485       0.965018   \n",
      "14      88.610388        0.017072    0.036188        0.991021       0.963812   \n",
      "15      92.247695        0.016465    0.034982        0.991289       0.965018   \n",
      "16      72.633571        0.016526    0.031363        0.991021       0.968637   \n",
      "17      97.384375        0.015011    0.021713        0.989815       0.978287   \n",
      "18      89.748179        0.015559    0.026538        0.989815       0.973462   \n",
      "19      98.415097        0.016136    0.025332        0.990753       0.974668   \n",
      "\n",
      "    precision  \n",
      "0    0.971983  \n",
      "1    0.969631  \n",
      "2    0.963753  \n",
      "3    0.963124  \n",
      "4    0.973742  \n",
      "5    0.959574  \n",
      "6    0.963753  \n",
      "7    0.980519  \n",
      "8    0.976035  \n",
      "9    0.963675  \n",
      "10   0.973568  \n",
      "11   0.967532  \n",
      "12   0.967880  \n",
      "13   0.973451  \n",
      "14   0.965142  \n",
      "15   0.955414  \n",
      "16   0.961538  \n",
      "17   0.988889  \n",
      "18   0.971861  \n",
      "19   0.969892  \n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, ensembles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
