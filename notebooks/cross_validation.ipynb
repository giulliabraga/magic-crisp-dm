{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../modules'))\n",
    "from best_models import models_to_cv\n",
    "from cross_validation import cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train_winsor_1_norm.csv')\n",
    "X_train = train_data.drop(columns='class')\n",
    "y_train = train_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    8659\n",
       "0    4574\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trials = {}\n",
    "model_names = ['KNN','DTR', 'SVM', 'RF', 'XGB', 'LGBM', 'MLP', 'LVQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models, ensembles = models_to_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model KNN\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:150: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 227, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0        KNN  fold_0  0.791830  0.952656  [[289, 169], [41, 825]]  0.887097   \n",
      "1        KNN  fold_1  0.785028  0.943418  [[287, 171], [49, 817]]  0.881338   \n",
      "2        KNN  fold_2  0.785542  0.942263  [[288, 170], [50, 816]]  0.881210   \n",
      "3        KNN  fold_3  0.803675  0.939954  [[305, 152], [52, 814]]  0.888646   \n",
      "4        KNN  fold_4  0.812428  0.939954  [[313, 144], [52, 814]]  0.892544   \n",
      "5        KNN  fold_5  0.783190  0.951501  [[281, 176], [42, 824]]  0.883173   \n",
      "6        KNN  fold_6  0.806683  0.950346  [[303, 154], [43, 823]]  0.893109   \n",
      "7        KNN  fold_7  0.809449  0.951501  [[305, 152], [42, 824]]  0.894680   \n",
      "8        KNN  fold_8  0.811120  0.952656  [[306, 151], [41, 825]]  0.895765   \n",
      "9        KNN  fold_9  0.811967  0.951445  [[308, 150], [42, 823]]  0.895539   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.002011        0.061648    0.158610        0.869930       0.841390   \n",
      "1       0.005618        0.054559    0.166163        0.870014       0.833837   \n",
      "2       0.000000        0.061081    0.166163        0.870854       0.833837   \n",
      "3       0.000000        0.063895    0.154195        0.869857       0.845805   \n",
      "4       0.003057        0.061424    0.148148        0.870193       0.851852   \n",
      "5       0.008013        0.066891    0.164777        0.868682       0.835223   \n",
      "6       0.000000        0.065629    0.148904        0.868850       0.851096   \n",
      "7       0.000000        0.061309    0.146636        0.867170       0.853364   \n",
      "8       0.000000        0.055278    0.145125        0.867170       0.854875   \n",
      "9       0.000590        0.056404    0.145125        0.868178       0.854875   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.829980  0.061648  \n",
      "1   0.826923  0.054559  \n",
      "2   0.827586  0.061081  \n",
      "3   0.842650  0.063895  \n",
      "4   0.849687  0.061424  \n",
      "5   0.824000  0.066891  \n",
      "6   0.842375  0.065629  \n",
      "7   0.844262  0.061309  \n",
      "8   0.845287  0.055278  \n",
      "9   0.845838  0.056404  \n",
      "Model LVQ\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0        LVQ  fold_0  0.721719  0.923788  [[238, 220], [66, 800]]  0.848356   \n",
      "1        LVQ  fold_1  0.719672  0.908776  [[243, 215], [79, 787]]  0.842612   \n",
      "2        LVQ  fold_2  0.723514  0.929561  [[237, 221], [61, 805]]  0.850951   \n",
      "3        LVQ  fold_3  0.742406  0.939954  [[249, 208], [52, 814]]  0.862288   \n",
      "4        LVQ  fold_4  0.740036  0.933025  [[250, 207], [58, 808]]  0.859117   \n",
      "5        LVQ  fold_5  0.661319  0.976905  [[158, 299], [20, 846]]  0.841372   \n",
      "6        LVQ  fold_6  0.747209  0.914550  [[265, 192], [74, 792]]  0.856216   \n",
      "7        LVQ  fold_7  0.734230  0.941109  [[241, 216], [51, 815]]  0.859251   \n",
      "8        LVQ  fold_8  0.730645  0.929561  [[243, 214], [61, 805]]  0.854111   \n",
      "9        LVQ  fold_9  0.735611  0.934104  [[246, 212], [57, 808]]  0.857294   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      40.874489        0.117445    0.216012        0.795281       0.783988   \n",
      "1      53.572030        0.170555    0.222054        0.799731       0.777946   \n",
      "2      38.271610        0.120934    0.212991        0.792678       0.787009   \n",
      "3      38.481040        0.124795    0.196523        0.792947       0.803477   \n",
      "4      80.397106        0.474019    0.200302        0.791016       0.799698   \n",
      "5     148.867328        0.470894    0.241119        0.756591       0.758881   \n",
      "6     152.803158        0.517831    0.201058        0.790764       0.798942   \n",
      "7      99.965252        0.120122    0.201814        0.787825       0.798186   \n",
      "8      36.677955        0.116941    0.207861        0.787741       0.792139   \n",
      "9      36.594285        0.117093    0.203326        0.787573       0.796674   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.784314  0.117445  \n",
      "1   0.785429  0.170555  \n",
      "2   0.784600  0.120934  \n",
      "3   0.796477  0.124795  \n",
      "4   0.796059  0.474019  \n",
      "5   0.738865  0.470894  \n",
      "6   0.804878  0.517831  \n",
      "7   0.790495  0.120122  \n",
      "8   0.789990  0.116941  \n",
      "9   0.792157  0.117093  \n",
      "Model DTR\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0        DTR  fold_0  0.801413  0.923788  [[311, 147], [66, 800]]  0.882515   \n",
      "1        DTR  fold_1  0.783117  0.913395  [[299, 159], [75, 791]]  0.871145   \n",
      "2        DTR  fold_2  0.781311  0.927252  [[291, 167], [63, 803]]  0.874728   \n",
      "3        DTR  fold_3  0.810698  0.894919  [[332, 125], [91, 775]]  0.877690   \n",
      "4        DTR  fold_4  0.811821  0.916859  [[323, 134], [72, 794]]  0.885173   \n",
      "5        DTR  fold_5  0.776051  0.908776  [[294, 163], [79, 787]]  0.866740   \n",
      "6        DTR  fold_6  0.769152  0.916859  [[284, 173], [72, 794]]  0.866339   \n",
      "7        DTR  fold_7  0.806775  0.933025  [[311, 146], [58, 808]]  0.887912   \n",
      "8        DTR  fold_8  0.802641  0.942263  [[303, 154], [50, 816]]  0.888889   \n",
      "9        DTR  fold_9  0.782746  0.921387  [[295, 163], [68, 797]]  0.873425   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.047118         0.00805    0.160876        0.897556       0.839124   \n",
      "1       0.051265         0.00000    0.176737        0.887732       0.823263   \n",
      "2       0.058570         0.00000    0.173716        0.883198       0.826284   \n",
      "3       0.058112         0.00000    0.163265        0.884887       0.836735   \n",
      "4       0.052902         0.00000    0.155707        0.879429       0.844293   \n",
      "5       0.050671         0.00000    0.182918        0.882284       0.817082   \n",
      "6       0.049757         0.00053    0.185185        0.875987       0.814815   \n",
      "7       0.050089         0.00000    0.154195        0.886398       0.845805   \n",
      "8       0.050619         0.00000    0.154195        0.867086       0.845805   \n",
      "9       0.056673         0.00000    0.174603        0.866163       0.825397   \n",
      "\n",
      "   precision    loacc  \n",
      "0   0.844773  0.00805  \n",
      "1   0.832632  0.00000  \n",
      "2   0.827835  0.00000  \n",
      "3   0.861111  0.00000  \n",
      "4   0.855603  0.00000  \n",
      "5   0.828421  0.00000  \n",
      "6   0.821096  0.00053  \n",
      "7   0.846960  0.00000  \n",
      "8   0.841237  0.00000  \n",
      "9   0.830208  0.00000  \n",
      "Model SVM\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                        CM  f1_score  \\\n",
      "0        SVM  fold_0  0.663670  0.916859   [[188, 270], [72, 794]]  0.822798   \n",
      "1        SVM  fold_1  0.682135  0.973441   [[179, 279], [23, 843]]  0.848089   \n",
      "2        SVM  fold_2  0.700074  0.930716   [[215, 243], [60, 806]]  0.841775   \n",
      "3        SVM  fold_3  0.676362  0.987298   [[167, 290], [11, 855]]  0.850323   \n",
      "4        SVM  fold_4  0.705512  0.909931   [[229, 228], [78, 788]]  0.837407   \n",
      "5        SVM  fold_5  0.678247  0.975751   [[174, 283], [21, 845]]  0.847543   \n",
      "6        SVM  fold_6  0.683565  0.990762    [[172, 285], [8, 858]]  0.854156   \n",
      "7        SVM  fold_7  0.659988  0.884527  [[199, 258], [100, 766]]  0.810582   \n",
      "8        SVM  fold_8  0.689370  0.982679   [[181, 276], [15, 851]]  0.853989   \n",
      "9        SVM  fold_9  0.671069  0.890173   [[207, 251], [95, 770]]  0.816543   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       7.001025        0.199951    0.258308        0.762029       0.741692   \n",
      "1       5.449045        0.191201    0.228097        0.795029       0.771903   \n",
      "2       5.482558        0.183347    0.228852        0.761021       0.771148   \n",
      "3       5.732741        0.201074    0.227513        0.779429       0.772487   \n",
      "4       5.464608        0.183370    0.231293        0.750798       0.768707   \n",
      "5       5.407621        0.183120    0.229781        0.781360       0.770219   \n",
      "6       5.441359        0.189111    0.221466        0.776490       0.778534   \n",
      "7       5.442403        0.191139    0.270597        0.737783       0.729403   \n",
      "8       5.440886        0.183208    0.219955        0.780772       0.780045   \n",
      "9       5.591449        0.183518    0.261527        0.735348       0.738473   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.746241  0.199951  \n",
      "1   0.751337  0.191201  \n",
      "2   0.768351  0.183347  \n",
      "3   0.746725  0.201074  \n",
      "4   0.775591  0.183370  \n",
      "5   0.749113  0.183120  \n",
      "6   0.750656  0.189111  \n",
      "7   0.748047  0.191139  \n",
      "8   0.755102  0.183208  \n",
      "9   0.754163  0.183518  \n",
      "Model RF\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0         RF  fold_0  0.831971  0.943418  [[330, 128], [49, 817]]  0.902264   \n",
      "1         RF  fold_1  0.841083  0.957275  [[332, 126], [37, 829]]  0.910489   \n",
      "2         RF  fold_2  0.826576  0.945727  [[324, 134], [47, 819]]  0.900495   \n",
      "3         RF  fold_3  0.854641  0.943418  [[350, 107], [49, 817]]  0.912849   \n",
      "4         RF  fold_4  0.849292  0.948037  [[343, 114], [45, 821]]  0.911716   \n",
      "5         RF  fold_5  0.838230  0.943418  [[335, 122], [49, 817]]  0.905263   \n",
      "6         RF  fold_6  0.852635  0.950346  [[345, 112], [43, 823]]  0.913937   \n",
      "7         RF  fold_7  0.848775  0.949192  [[342, 115], [44, 822]]  0.911814   \n",
      "8         RF  fold_8  0.858227  0.954965  [[348, 109], [39, 827]]  0.917869   \n",
      "9         RF  fold_9  0.852681  0.943353  [[349, 109], [49, 816]]  0.911732   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      22.634897        0.124945    0.133686        0.969939       0.866314   \n",
      "1      37.065829        0.295920    0.123112        0.970694       0.876888   \n",
      "2      23.378320        0.125258    0.136707        0.970023       0.863293   \n",
      "3      28.893621        0.155571    0.117914        0.970781       0.882086   \n",
      "4      23.979026        0.119526    0.120181        0.969521       0.879819   \n",
      "5      22.493655        0.115536    0.129252        0.970781       0.870748   \n",
      "6      22.529516        0.133280    0.117158        0.971117       0.882842   \n",
      "7      23.867475        0.103699    0.120181        0.970697       0.879819   \n",
      "8      22.678588        0.117509    0.111867        0.968682       0.888133   \n",
      "9      22.343509        0.102944    0.119426        0.970781       0.880574   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.864550  0.124945  \n",
      "1   0.868063  0.295920  \n",
      "2   0.859391  0.125258  \n",
      "3   0.884199  0.155571  \n",
      "4   0.878075  0.119526  \n",
      "5   0.870075  0.115536  \n",
      "6   0.880214  0.133280  \n",
      "7   0.877268  0.103699  \n",
      "8   0.883547  0.117509  \n",
      "9   0.882162  0.102944  \n",
      "Model XGB\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0        XGB  fold_0  0.858235  0.945727  [[353, 105], [47, 819]]  0.915084   \n",
      "1        XGB  fold_1  0.855023  0.948037  [[349, 109], [45, 821]]  0.914254   \n",
      "2        XGB  fold_2  0.840705  0.943418  [[338, 120], [49, 817]]  0.906267   \n",
      "3        XGB  fold_3  0.865400  0.936490   [[363, 94], [55, 811]]  0.915867   \n",
      "4        XGB  fold_4  0.861084  0.938799   [[358, 99], [53, 813]]  0.914511   \n",
      "5        XGB  fold_5  0.841786  0.933025  [[343, 114], [58, 808]]  0.903803   \n",
      "6        XGB  fold_6  0.859018  0.943418  [[354, 103], [49, 817]]  0.914894   \n",
      "7        XGB  fold_7  0.864488  0.943418   [[359, 98], [49, 817]]  0.917462   \n",
      "8        XGB  fold_8  0.870718  0.951501   [[361, 96], [42, 824]]  0.922732   \n",
      "9        XGB  fold_9  0.860194  0.938728  [[358, 100], [53, 812]]  0.913900   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      25.610032        0.019566    0.114804        0.965236       0.885196   \n",
      "1      26.075234        0.016779    0.116314        0.965236       0.883686   \n",
      "2      25.816069        0.016356    0.127644        0.965908       0.872356   \n",
      "3      25.620949        0.016743    0.112623        0.965911       0.887377   \n",
      "4      25.345868        0.019223    0.114890        0.966079       0.885110   \n",
      "5      25.679259        0.016485    0.130008        0.966583       0.869992   \n",
      "6      26.137627        0.017143    0.114890        0.964400       0.885110   \n",
      "7      25.712451        0.017625    0.111111        0.966751       0.888889   \n",
      "8      25.462593        0.016873    0.104308        0.965491       0.895692   \n",
      "9      25.554243        0.016687    0.115646        0.966079       0.884354   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.886364  0.019566  \n",
      "1   0.882796  0.016779  \n",
      "2   0.871932  0.016356  \n",
      "3   0.896133  0.016743  \n",
      "4   0.891447  0.019223  \n",
      "5   0.876356  0.016485  \n",
      "6   0.888043  0.017143  \n",
      "7   0.892896  0.017625  \n",
      "8   0.895652  0.016873  \n",
      "9   0.890351  0.016687  \n",
      "Model LGBM\n",
      "Fold: 0\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 4116\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11909, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654379 -> initscore=0.638344\n",
      "[LightGBM] [Info] Start training from score 0.638344\n",
      "Fold: 1\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 4116\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11909, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654379 -> initscore=0.638344\n",
      "[LightGBM] [Info] Start training from score 0.638344\n",
      "Fold: 2\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 4116\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11909, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654379 -> initscore=0.638344\n",
      "[LightGBM] [Info] Start training from score 0.638344\n",
      "Fold: 3\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 4117\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11910, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654324 -> initscore=0.638101\n",
      "[LightGBM] [Info] Start training from score 0.638101\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Fold: 4\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 4117\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11910, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654324 -> initscore=0.638101\n",
      "[LightGBM] [Info] Start training from score 0.638101\n",
      "Fold: 5\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 4117\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11910, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654324 -> initscore=0.638101\n",
      "[LightGBM] [Info] Start training from score 0.638101\n",
      "Fold: 6\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 4117\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11910, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654324 -> initscore=0.638101\n",
      "[LightGBM] [Info] Start training from score 0.638101\n",
      "Fold: 7\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 4117\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11910, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654324 -> initscore=0.638101\n",
      "[LightGBM] [Info] Start training from score 0.638101\n",
      "Fold: 8\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 4117\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11910, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654324 -> initscore=0.638101\n",
      "[LightGBM] [Info] Start training from score 0.638101\n",
      "Fold: 9\n",
      "[LightGBM] [Info] Number of positive: 7794, number of negative: 4116\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 11910, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654408 -> initscore=0.638472\n",
      "[LightGBM] [Info] Start training from score 0.638472\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0       LGBM  fold_0  0.854319  0.942263  [[351, 107], [50, 816]]  0.912241   \n",
      "1       LGBM  fold_1  0.849113  0.951501  [[342, 116], [42, 824]]  0.912514   \n",
      "2       LGBM  fold_2  0.841345  0.946882  [[337, 121], [46, 820]]  0.907582   \n",
      "3       LGBM  fold_3  0.861601  0.937644   [[359, 98], [54, 812]]  0.914414   \n",
      "4       LGBM  fold_4  0.861541  0.935335   [[360, 97], [56, 810]]  0.913706   \n",
      "5       LGBM  fold_5  0.841968  0.939954  [[340, 117], [52, 814]]  0.905954   \n",
      "6       LGBM  fold_6  0.856252  0.942263  [[352, 105], [50, 816]]  0.913262   \n",
      "7       LGBM  fold_7  0.858501  0.944573  [[353, 104], [48, 818]]  0.914989   \n",
      "8       LGBM  fold_8  0.862695  0.937644   [[360, 97], [54, 812]]  0.914930   \n",
      "9       LGBM  fold_9  0.860258  0.941040  [[357, 101], [51, 814]]  0.914607   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.719302        0.001837    0.118580        0.971198       0.881420   \n",
      "1       0.749198        0.004865    0.119335        0.972794       0.880665   \n",
      "2       0.724674        0.005307    0.126133        0.972794       0.873867   \n",
      "3       0.796374        0.008825    0.114890        0.972040       0.885110   \n",
      "4       0.834576        0.008405    0.115646        0.971201       0.884354   \n",
      "5       0.707312        0.008108    0.127740        0.972208       0.872260   \n",
      "6       0.718820        0.000000    0.117158        0.971285       0.882842   \n",
      "7       0.725014        0.003818    0.114890        0.969941       0.885110   \n",
      "8       0.741539        0.008633    0.114135        0.971453       0.885865   \n",
      "9       0.699990        0.003095    0.114890        0.969773       0.885110   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.884074  0.001837  \n",
      "1   0.876596  0.004865  \n",
      "2   0.871413  0.005307  \n",
      "3   0.892308  0.008825  \n",
      "4   0.893054  0.008405  \n",
      "5   0.874329  0.008108  \n",
      "6   0.885993  0.000000  \n",
      "7   0.887202  0.003818  \n",
      "8   0.893289  0.008633  \n",
      "9   0.889617  0.003095  \n",
      "Model MLP\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0        MLP  fold_0  0.849070  0.909931   [[361, 97], [78, 788]]  0.900057   \n",
      "1        MLP  fold_1  0.831583  0.949192  [[327, 131], [44, 822]]  0.903793   \n",
      "2        MLP  fold_2  0.820277  0.954965  [[314, 144], [39, 827]]  0.900381   \n",
      "3        MLP  fold_3  0.856284  0.922633   [[361, 96], [67, 799]]  0.907439   \n",
      "4        MLP  fold_4  0.852941  0.920323   [[359, 98], [69, 797]]  0.905168   \n",
      "5        MLP  fold_5  0.824584  0.944573  [[322, 135], [48, 818]]  0.899395   \n",
      "6        MLP  fold_6  0.851725  0.915704   [[360, 97], [73, 793]]  0.903189   \n",
      "7        MLP  fold_7  0.854367  0.953811  [[345, 112], [40, 826]]  0.915743   \n",
      "8        MLP  fold_8  0.865155  0.968822  [[348, 109], [27, 839]]  0.925028   \n",
      "9        MLP  fold_9  0.848891  0.924855  [[354, 104], [65, 800]]  0.904466   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      27.325076        0.001133    0.132175        0.882610       0.867825   \n",
      "1      37.449173        0.000000    0.132175        0.884877       0.867825   \n",
      "2      50.716381        0.000000    0.138218        0.885969       0.861782   \n",
      "3      30.704167        0.000000    0.123205        0.881192       0.876795   \n",
      "4      28.929408        0.007631    0.126228        0.880856       0.873772   \n",
      "5      45.668117        0.000000    0.138322        0.886146       0.861678   \n",
      "6    1391.864173        0.002000    0.128496        0.885139       0.871504   \n",
      "7      21.420906        0.002002    0.114890        0.886902       0.885110   \n",
      "8      20.455529        0.000000    0.102797        0.885978       0.897203   \n",
      "9      69.042219        0.004176    0.127740        0.884719       0.872260   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.890395  0.001133  \n",
      "1   0.862539  0.000000  \n",
      "2   0.851699  0.000000  \n",
      "3   0.892737  0.000000  \n",
      "4   0.890503  0.007631  \n",
      "5   0.858342  0.000000  \n",
      "6   0.891011  0.002000  \n",
      "7   0.880597  0.002002  \n",
      "8   0.885021  0.000000  \n",
      "9   0.884956  0.004176  \n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Heterog√™neo\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "    model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0  Heterog√™neo  fold_0  0.830229  0.959584  [[321, 137], [35, 831]]  0.906216   \n",
      "1  Heterog√™neo  fold_1  0.834018  0.958430  [[325, 133], [36, 830]]  0.907600   \n",
      "2  Heterog√™neo  fold_2  0.824518  0.950346  [[320, 138], [43, 823]]  0.900930   \n",
      "3  Heterog√™neo  fold_3  0.852696  0.952656  [[344, 113], [41, 825]]  0.914634   \n",
      "4  Heterog√™neo  fold_4  0.851085  0.953811  [[342, 115], [40, 826]]  0.914222   \n",
      "5  Heterog√™neo  fold_5  0.828109  0.953811  [[321, 136], [40, 826]]  0.903720   \n",
      "6  Heterog√™neo  fold_6  0.847802  0.953811  [[339, 118], [40, 826]]  0.912707   \n",
      "7  Heterog√™neo  fold_7  0.851145  0.956120  [[341, 116], [38, 828]]  0.914917   \n",
      "8  Heterog√™neo  fold_8  0.854428  0.956120  [[344, 113], [38, 828]]  0.916436   \n",
      "9  Heterog√™neo  fold_9  0.848122  0.956069  [[339, 119], [38, 827]]  0.913308   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      41.808224        0.402833    0.129909        0.945000       0.870091   \n",
      "1      42.198912        0.120861    0.127644        0.946931       0.872356   \n",
      "2      27.271462        0.309419    0.136707        0.944664       0.863293   \n",
      "3      48.290732        0.224681    0.116402        0.945340       0.883598   \n",
      "4      50.405165        0.134013    0.117158        0.945256       0.882842   \n",
      "5      61.369048        0.249804    0.133031        0.944332       0.866969   \n",
      "6      65.609034        0.219279    0.119426        0.945004       0.880574   \n",
      "7      65.707381        0.236109    0.116402        0.946432       0.883598   \n",
      "8      66.074853        0.240141    0.114135        0.944920       0.885865   \n",
      "9      65.346719        0.251354    0.118670        0.947019       0.881330   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.858471  0.402833  \n",
      "1   0.861890  0.120861  \n",
      "2   0.856400  0.309419  \n",
      "3   0.879531  0.224681  \n",
      "4   0.877790  0.134013  \n",
      "5   0.858628  0.249804  \n",
      "6   0.875000  0.219279  \n",
      "7   0.877119  0.236109  \n",
      "8   0.879915  0.240141  \n",
      "9   0.874207  0.251354  \n",
      "Model ANNs\n",
      "Fold: 0\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n",
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0       ANNs  fold_0  0.837871  0.959584  [[328, 130], [35, 831]]  0.909688   \n",
      "1       ANNs  fold_1  0.834732  0.944573  [[332, 126], [48, 818]]  0.903867   \n",
      "2       ANNs  fold_2  0.834732  0.944573  [[332, 126], [48, 818]]  0.903867   \n",
      "3       ANNs  fold_3  0.846526  0.946882  [[341, 116], [46, 820]]  0.910100   \n",
      "4       ANNs  fold_4  0.857802  0.938799  [[355, 102], [53, 813]]  0.912970   \n",
      "5       ANNs  fold_5  0.824523  0.942263  [[323, 134], [50, 816]]  0.898678   \n",
      "6       ANNs  fold_6  0.855037  0.937644  [[353, 104], [54, 812]]  0.911336   \n",
      "7       ANNs  fold_7  0.856616  0.956120  [[346, 111], [38, 828]]  0.917452   \n",
      "8       ANNs  fold_8  0.866463  0.956120  [[355, 102], [38, 828]]  0.922049   \n",
      "9       ANNs  fold_9  0.849406  0.943353  [[346, 112], [49, 816]]  0.910206   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0     134.785218        0.015625    0.124622        0.882778       0.875378   \n",
      "1     172.934863        0.000000    0.131420        0.886976       0.868580   \n",
      "2      44.692264        0.002123    0.131420        0.886556       0.868580   \n",
      "3      45.581607        0.005865    0.122449        0.886398       0.877551   \n",
      "4      45.660719        0.005327    0.117158        0.884215       0.882842   \n",
      "5      45.460908        0.008921    0.139078        0.886146       0.860922   \n",
      "6      58.833076        0.005385    0.119426        0.887573       0.880574   \n",
      "7      39.918807        0.008519    0.112623        0.885223       0.887377   \n",
      "8      43.159396        0.003190    0.105820        0.884131       0.894180   \n",
      "9      55.944865        0.009228    0.121693        0.887825       0.878307   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.864724  0.015625  \n",
      "1   0.866525  0.000000  \n",
      "2   0.866525  0.002123  \n",
      "3   0.876068  0.005865  \n",
      "4   0.888525  0.005327  \n",
      "5   0.858947  0.008921  \n",
      "6   0.886463  0.005385  \n",
      "7   0.881789  0.008519  \n",
      "8   0.890323  0.003190  \n",
      "9   0.879310  0.009228  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, ensembles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model KNN\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                        CM  f1_score  \\\n",
      "0        KNN  fold_0  0.792614  0.801386   [[359, 99], [172, 694]]  0.836649   \n",
      "1        KNN  fold_1  0.800770  0.800231   [[367, 91], [173, 693]]  0.840000   \n",
      "2        KNN  fold_2  0.789717  0.815242  [[350, 108], [160, 706]]  0.840476   \n",
      "3        KNN  fold_3  0.799398  0.797921   [[366, 91], [175, 691]]  0.838592   \n",
      "4        KNN  fold_4  0.805386  0.796767   [[372, 85], [176, 690]]  0.840951   \n",
      "5        KNN  fold_5  0.787880  0.796767  [[356, 101], [176, 690]]  0.832830   \n",
      "6        KNN  fold_6  0.820429  0.807159   [[381, 76], [167, 699]]  0.851920   \n",
      "7        KNN  fold_7  0.816234  0.814088   [[374, 83], [161, 705]]  0.852479   \n",
      "8        KNN  fold_8  0.813985  0.811778   [[373, 84], [163, 703]]  0.850575   \n",
      "9        KNN  fold_9  0.810416  0.797688   [[377, 81], [175, 690]]  0.843521   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.007318        0.057186    0.204683        0.875202       0.795317   \n",
      "1       0.000000        0.051238    0.199396        0.880051       0.800604   \n",
      "2       0.000000        0.043351    0.202417        0.878347       0.797583   \n",
      "3       0.006268        0.041206    0.201058        0.879051       0.798942   \n",
      "4       0.000000        0.050405    0.197279        0.877627       0.802721   \n",
      "5       0.000000        0.045348    0.209373        0.877563       0.790627   \n",
      "6       0.000000        0.049809    0.183673        0.879424       0.816327   \n",
      "7       0.001007        0.050369    0.184429        0.875748       0.815571   \n",
      "8       0.000633        0.044100    0.186697        0.877201       0.813303   \n",
      "9       0.000000        0.041492    0.193500        0.878088       0.806500   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.875158  0.057186  \n",
      "1   0.883929  0.051238  \n",
      "2   0.867322  0.043351  \n",
      "3   0.883632  0.041206  \n",
      "4   0.890323  0.050405  \n",
      "5   0.872314  0.045348  \n",
      "6   0.901935  0.049809  \n",
      "7   0.894670  0.050369  \n",
      "8   0.893266  0.044100  \n",
      "9   0.894942  0.041492  \n",
      "Model LVQ\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                     CM  f1_score  \\\n",
      "0        LVQ  fold_0  0.515589  0.031178  [[458, 0], [839, 27]]  0.060470   \n",
      "1        LVQ  fold_1  0.515012  0.030023  [[458, 0], [840, 26]]  0.058296   \n",
      "2        LVQ  fold_2  0.517961  0.038106  [[457, 1], [833, 33]]  0.073333   \n",
      "3        LVQ  fold_3  0.523277  0.053118  [[454, 3], [820, 46]]  0.100546   \n",
      "4        LVQ  fold_4  0.519752  0.043880  [[455, 2], [828, 38]]  0.083885   \n",
      "5        LVQ  fold_5  0.514555  0.033487  [[455, 2], [837, 29]]  0.064660   \n",
      "6        LVQ  fold_6  0.506928  0.013857  [[457, 0], [854, 12]]  0.027335   \n",
      "7        LVQ  fold_7  0.512185  0.026559  [[456, 1], [843, 23]]  0.051685   \n",
      "8        LVQ  fold_8  0.519630  0.039261  [[457, 0], [832, 34]]  0.075556   \n",
      "9        LVQ  fold_9  0.518048  0.040462  [[456, 2], [830, 35]]  0.077605   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      28.559797        0.061414    0.633686        0.489342       0.366314   \n",
      "1      27.356409        0.063381    0.634441        0.488700       0.365559   \n",
      "2      27.121573        0.071467    0.629909        0.491547       0.370091   \n",
      "3      27.817550        0.068181    0.622071        0.502392       0.377929   \n",
      "4      27.914532        0.057819    0.627362        0.492120       0.372638   \n",
      "5      28.525902        0.061076    0.634165        0.491230       0.365835   \n",
      "6      28.018731        0.073692    0.645503        0.487821       0.354497   \n",
      "7      27.788723        0.062951    0.637944        0.491024       0.362056   \n",
      "8      28.007945        0.077849    0.628874        0.497580       0.371126   \n",
      "9      27.144031        0.065990    0.628874        0.493153       0.371126   \n",
      "\n",
      "   precision     loacc  \n",
      "0   1.000000  0.061414  \n",
      "1   1.000000  0.063381  \n",
      "2   0.970588  0.071467  \n",
      "3   0.938776  0.068181  \n",
      "4   0.950000  0.057819  \n",
      "5   0.935484  0.061076  \n",
      "6   1.000000  0.073692  \n",
      "7   0.958333  0.062951  \n",
      "8   1.000000  0.077849  \n",
      "9   0.945946  0.065990  \n",
      "Model DTR\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                        CM  f1_score  \\\n",
      "0        DTR  fold_0  0.821367  0.834873   [[370, 88], [143, 723]]  0.862254   \n",
      "1        DTR  fold_1  0.806670  0.816397   [[365, 93], [159, 707]]  0.848739   \n",
      "2        DTR  fold_2  0.781740  0.842956  [[330, 128], [136, 730]]  0.846868   \n",
      "3        DTR  fold_3  0.802923  0.807159   [[365, 92], [167, 699]]  0.843693   \n",
      "4        DTR  fold_4  0.802496  0.832564  [[353, 104], [145, 721]]  0.852750   \n",
      "5        DTR  fold_5  0.784384  0.809469  [[347, 110], [165, 701]]  0.836017   \n",
      "6        DTR  fold_6  0.817236  0.831409   [[367, 90], [146, 720]]  0.859189   \n",
      "7        DTR  fold_7  0.806904  0.812933   [[366, 91], [162, 704]]  0.847682   \n",
      "8        DTR  fold_8  0.805383  0.838337  [[353, 104], [140, 726]]  0.856132   \n",
      "9        DTR  fold_9  0.784283  0.858960  [[325, 133], [122, 743]]  0.853532   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.034389        0.000000    0.174471        0.844846       0.825529   \n",
      "1       0.038135        0.000000    0.190332        0.835323       0.809668   \n",
      "2       0.035234        0.000000    0.199396        0.821680       0.800604   \n",
      "3       0.040385        0.000000    0.195767        0.831683       0.804233   \n",
      "4       0.036356        0.000000    0.188209        0.837958       0.811791   \n",
      "5       0.053779        0.000000    0.207861        0.823394       0.792139   \n",
      "6       0.042441        0.000000    0.178382        0.823510       0.821618   \n",
      "7       0.033692        0.000000    0.191232        0.819068       0.808768   \n",
      "8       0.041564        0.000000    0.184429        0.823699       0.815571   \n",
      "9       0.039063        0.000996    0.192744        0.812769       0.807256   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.891492  0.000000  \n",
      "1   0.883750  0.000000  \n",
      "2   0.850816  0.000000  \n",
      "3   0.883692  0.000000  \n",
      "4   0.873939  0.000000  \n",
      "5   0.864365  0.000000  \n",
      "6   0.888889  0.000000  \n",
      "7   0.885535  0.000000  \n",
      "8   0.874699  0.000000  \n",
      "9   0.848174  0.000996  \n",
      "Model SVM\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                        CM  f1_score  \\\n",
      "0        SVM  fold_0  0.709877  0.849885  [[261, 197], [130, 736]]  0.818232   \n",
      "1        SVM  fold_1  0.739270  0.886836   [[271, 187], [98, 768]]  0.843493   \n",
      "2        SVM  fold_2  0.689913  0.958430   [[193, 265], [36, 830]]  0.846507   \n",
      "3        SVM  fold_3  0.728425  0.949192   [[232, 225], [44, 822]]  0.859383   \n",
      "4        SVM  fold_4  0.718161  0.808314  [[287, 170], [166, 700]]  0.806452   \n",
      "5        SVM  fold_5  0.671988  0.945727   [[182, 275], [47, 819]]  0.835714   \n",
      "6        SVM  fold_6  0.780248  0.860277  [[320, 137], [121, 745]]  0.852403   \n",
      "7        SVM  fold_7  0.772861  0.891455   [[299, 158], [94, 772]]  0.859688   \n",
      "8        SVM  fold_8  0.643943  0.856813  [[197, 260], [124, 742]]  0.794433   \n",
      "9        SVM  fold_9  0.723605  0.973410   [[217, 241], [23, 842]]  0.864476   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       4.438482        0.139509    0.246979        0.666622       0.753021   \n",
      "1       4.111155        0.102998    0.215257        0.711192       0.784743   \n",
      "2       5.439610        0.347159    0.227341        0.646402       0.772659   \n",
      "3      12.337897        0.354890    0.203326        0.665993       0.796674   \n",
      "4      12.496075        0.298402    0.253968        0.633149       0.746032   \n",
      "5      12.439780        0.298269    0.243386        0.637750       0.756614   \n",
      "6      12.729841        0.392681    0.195011        0.698964       0.804989   \n",
      "7       5.189460        0.127029    0.190476        0.701405       0.809524   \n",
      "8       4.922582        0.139963    0.290249        0.588856       0.709751   \n",
      "9       5.934973        0.392121    0.199546        0.665145       0.800454   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.788853  0.139509  \n",
      "1   0.804188  0.102998  \n",
      "2   0.757991  0.347159  \n",
      "3   0.785100  0.354890  \n",
      "4   0.804598  0.298402  \n",
      "5   0.748629  0.298269  \n",
      "6   0.844671  0.392681  \n",
      "7   0.830108  0.127029  \n",
      "8   0.740519  0.139963  \n",
      "9   0.777470  0.392121  \n",
      "Model RF\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0         RF  fold_0  0.856921  0.877598  [[383, 75], [106, 760]]  0.893592   \n",
      "1         RF  fold_1  0.857163  0.906467   [[370, 88], [81, 785]]  0.902818   \n",
      "2         RF  fold_2  0.842782  0.899538   [[360, 98], [87, 779]]  0.893861   \n",
      "3         RF  fold_3  0.854857  0.889145   [[375, 82], [96, 770]]  0.896391   \n",
      "4         RF  fold_4  0.845983  0.884527  [[369, 88], [100, 766]]  0.890698   \n",
      "5         RF  fold_5  0.842124  0.883372  [[366, 91], [101, 765]]  0.888502   \n",
      "6         RF  fold_6  0.873061  0.894919   [[389, 68], [91, 775]]  0.906963   \n",
      "7         RF  fold_7  0.866315  0.887991   [[386, 71], [97, 769]]  0.901524   \n",
      "8         RF  fold_8  0.860510  0.896074   [[377, 80], [90, 776]]  0.901278   \n",
      "9         RF  fold_9  0.859356  0.889017   [[380, 78], [96, 769]]  0.898364   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      35.320591        0.074948    0.136707        0.989949       0.863293   \n",
      "1      33.564839        0.204371    0.127644        0.989611       0.872356   \n",
      "2      39.031210        0.141296    0.139728        0.989857       0.860272   \n",
      "3      38.407614        0.220059    0.134543        0.989825       0.865457   \n",
      "4      39.005819        0.238964    0.142101        0.989494       0.857899   \n",
      "5      39.321030        0.136409    0.145125        0.989207       0.854875   \n",
      "6      39.955750        0.202674    0.120181        0.989773       0.879819   \n",
      "7      39.127448        0.135343    0.126984        0.991192       0.873016   \n",
      "8      29.986882        0.064629    0.128496        0.989918       0.871504   \n",
      "9      39.212678        0.198634    0.131519        0.990803       0.868481   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.910180  0.074948  \n",
      "1   0.899198  0.204371  \n",
      "2   0.888255  0.141296  \n",
      "3   0.903756  0.220059  \n",
      "4   0.896956  0.238964  \n",
      "5   0.893692  0.136409  \n",
      "6   0.919336  0.202674  \n",
      "7   0.915476  0.135343  \n",
      "8   0.906542  0.064629  \n",
      "9   0.907910  0.198634  \n",
      "Model XGB\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0        XGB  fold_0  0.867964  0.882217  [[391, 67], [102, 764]]  0.900412   \n",
      "1        XGB  fold_1  0.863073  0.903002   [[377, 81], [84, 782]]  0.904569   \n",
      "2        XGB  fold_2  0.851001  0.900693   [[367, 91], [86, 780]]  0.898100   \n",
      "3        XGB  fold_3  0.857380  0.881062  [[381, 76], [103, 763]]  0.895015   \n",
      "4        XGB  fold_4  0.860146  0.882217  [[383, 74], [102, 764]]  0.896714   \n",
      "5        XGB  fold_5  0.853004  0.881062  [[377, 80], [103, 763]]  0.892920   \n",
      "6        XGB  fold_6  0.870691  0.887991   [[390, 67], [97, 769]]  0.903643   \n",
      "7        XGB  fold_7  0.858292  0.874134  [[385, 72], [109, 757]]  0.893215   \n",
      "8        XGB  fold_8  0.867074  0.896074   [[383, 74], [90, 776]]  0.904429   \n",
      "9        XGB  fold_9  0.863723  0.889017   [[384, 74], [96, 769]]  0.900468   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      40.264711        0.020238    0.127644        0.958176       0.872356   \n",
      "1      42.637511        0.033324    0.124622        0.957161       0.875378   \n",
      "2      22.335132        0.010586    0.133686        0.958615       0.866314   \n",
      "3      19.752549        0.009895    0.135299        0.960043       0.864701   \n",
      "4      19.123480        0.010296    0.133031        0.956695       0.866969   \n",
      "5      19.681568        0.009120    0.138322        0.958041       0.861678   \n",
      "6      18.623626        0.010897    0.123961        0.957543       0.876039   \n",
      "7      18.229464        0.010021    0.136810        0.958583       0.863190   \n",
      "8      18.816380        0.007586    0.123961        0.956513       0.876039   \n",
      "9      28.015431        0.022238    0.128496        0.956498       0.871504   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.919374  0.020238  \n",
      "1   0.906141  0.033324  \n",
      "2   0.895522  0.010586  \n",
      "3   0.909416  0.009895  \n",
      "4   0.911695  0.010296  \n",
      "5   0.905101  0.009120  \n",
      "6   0.919856  0.010897  \n",
      "7   0.913148  0.010021  \n",
      "8   0.912941  0.007586  \n",
      "9   0.912218  0.022238  \n",
      "Model LGBM\n",
      "Fold: 0\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7031\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14824, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525702 -> initscore=0.102897\n",
      "[LightGBM] [Info] Start training from score 0.102897\n",
      "Fold: 1\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14823, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525737 -> initscore=0.103039\n",
      "[LightGBM] [Info] Start training from score 0.103039\n",
      "Fold: 2\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 6995\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14788, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.526981 -> initscore=0.108030\n",
      "[LightGBM] [Info] Start training from score 0.108030\n",
      "Fold: 3\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7048\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14841, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525099 -> initscore=0.100482\n",
      "[LightGBM] [Info] Start training from score 0.100482\n",
      "Fold: 4\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7055\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14848, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524852 -> initscore=0.099489\n",
      "[LightGBM] [Info] Start training from score 0.099489\n",
      "Fold: 5\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7031\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14824, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525702 -> initscore=0.102897\n",
      "[LightGBM] [Info] Start training from score 0.102897\n",
      "Fold: 6\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14862, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524357 -> initscore=0.097507\n",
      "[LightGBM] [Info] Start training from score 0.097507\n",
      "Fold: 7\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7080\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14873, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523970 -> initscore=0.095952\n",
      "[LightGBM] [Info] Start training from score 0.095952\n",
      "Fold: 8\n",
      "[LightGBM] [Info] Number of positive: 7793, number of negative: 7085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14878, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523794 -> initscore=0.095246\n",
      "[LightGBM] [Info] Start training from score 0.095246\n",
      "Fold: 9\n",
      "[LightGBM] [Info] Number of positive: 7794, number of negative: 7102\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 14896, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523228 -> initscore=0.092978\n",
      "[LightGBM] [Info] Start training from score 0.092978\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0       LGBM  fold_0  0.865393  0.887991   [[386, 72], [97, 769]]  0.900996   \n",
      "1       LGBM  fold_1  0.860438  0.906467   [[373, 85], [81, 785]]  0.904378   \n",
      "2       LGBM  fold_2  0.842205  0.898383   [[360, 98], [88, 778]]  0.893226   \n",
      "3       LGBM  fold_3  0.866133  0.881062  [[389, 68], [103, 763]]  0.899234   \n",
      "4       LGBM  fold_4  0.860723  0.883372  [[383, 74], [101, 765]]  0.897361   \n",
      "5       LGBM  fold_5  0.854736  0.884527  [[377, 80], [100, 766]]  0.894860   \n",
      "6       LGBM  fold_6  0.865281  0.890300   [[384, 73], [95, 771]]  0.901754   \n",
      "7       LGBM  fold_7  0.861058  0.875289  [[387, 70], [108, 758]]  0.894923   \n",
      "8       LGBM  fold_8  0.870082  0.906467   [[381, 76], [81, 785]]  0.909091   \n",
      "9       LGBM  fold_9  0.871686  0.900578   [[386, 72], [86, 779]]  0.907925   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       4.540364        0.011463    0.127644        0.959997       0.872356   \n",
      "1       3.480144        0.020066    0.125378        0.960062       0.874622   \n",
      "2       4.374707        0.019683    0.140483        0.957330       0.859517   \n",
      "3       5.976032        0.020984    0.129252        0.958830       0.870748   \n",
      "4       4.693427        0.023162    0.132275        0.957233       0.867725   \n",
      "5       3.684203        0.013323    0.136054        0.960402       0.863946   \n",
      "6       4.684942        0.020037    0.126984        0.960772       0.873016   \n",
      "7       3.614685        0.020165    0.134543        0.958852       0.865457   \n",
      "8       4.505909        0.011486    0.118670        0.960344       0.881330   \n",
      "9       3.228973        0.025777    0.119426        0.959788       0.880574   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.914388  0.011463  \n",
      "1   0.902299  0.020066  \n",
      "2   0.888128  0.019683  \n",
      "3   0.918171  0.020984  \n",
      "4   0.911800  0.023162  \n",
      "5   0.905437  0.013323  \n",
      "6   0.913507  0.020037  \n",
      "7   0.915459  0.020165  \n",
      "8   0.911731  0.011486  \n",
      "9   0.915394  0.025777  \n",
      "Model MLP\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n",
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n",
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0        MLP  fold_0  0.841721  0.840647  [[386, 72], [138, 728]]  0.873950   \n",
      "1        MLP  fold_1  0.847232  0.862587  [[381, 77], [119, 747]]  0.884024   \n",
      "2        MLP  fold_2  0.837092  0.851039  [[377, 81], [129, 737]]  0.875297   \n",
      "3        MLP  fold_3  0.859661  0.863741  [[391, 66], [118, 748]]  0.890476   \n",
      "4        MLP  fold_4  0.853402  0.833718  [[399, 58], [144, 722]]  0.877278   \n",
      "5        MLP  fold_5  0.847262  0.849885  [[386, 71], [130, 736]]  0.879857   \n",
      "6        MLP  fold_6  0.836566  0.817552  [[391, 66], [158, 708]]  0.863415   \n",
      "7        MLP  fold_7  0.843890  0.825635  [[394, 63], [151, 715]]  0.869830   \n",
      "8        MLP  fold_8  0.874701  0.915704   [[381, 76], [73, 793]]  0.914121   \n",
      "9        MLP  fold_9  0.843233  0.819653  [[397, 61], [156, 709]]  0.867278   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      93.890018        0.050328    0.158610        0.831287       0.841390   \n",
      "1      52.977385        0.006170    0.148036        0.839034       0.851964   \n",
      "2      97.049232        0.027108    0.158610        0.836151       0.841390   \n",
      "3      88.885319        0.199594    0.139078        0.837073       0.860922   \n",
      "4     112.005427        0.025999    0.152683        0.837554       0.847317   \n",
      "5      35.535191        0.001004    0.151927        0.827037       0.848073   \n",
      "6     119.945965        0.164524    0.169312        0.833804       0.830688   \n",
      "7     162.439954        0.140238    0.161754        0.830565       0.838246   \n",
      "8      94.968576        0.015614    0.112623        0.830286       0.887377   \n",
      "9      24.123662        0.014056    0.164021        0.830290       0.835979   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.910000  0.050328  \n",
      "1   0.906553  0.006170  \n",
      "2   0.900978  0.027108  \n",
      "3   0.918919  0.199594  \n",
      "4   0.925641  0.025999  \n",
      "5   0.912020  0.001004  \n",
      "6   0.914729  0.164524  \n",
      "7   0.919023  0.140238  \n",
      "8   0.912543  0.015614  \n",
      "9   0.920779  0.014056  \n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, best_models, use_adasyn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Heterog√™neo\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "    model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0  Heterog√™neo  fold_0  0.846193  0.884527  [[370, 88], [100, 766]]  0.890698   \n",
      "1  Heterog√™neo  fold_1  0.850613  0.906467   [[364, 94], [81, 785]]  0.899713   \n",
      "2  Heterog√™neo  fold_2  0.841375  0.887991   [[364, 94], [97, 769]]  0.889532   \n",
      "3  Heterog√™neo  fold_3  0.857897  0.879908  [[382, 75], [104, 762]]  0.894891   \n",
      "4  Heterog√™neo  fold_4  0.856650  0.894919   [[374, 83], [91, 775]]  0.899072   \n",
      "5  Heterog√™neo  fold_5  0.841607  0.884527  [[365, 92], [100, 766]]  0.888631   \n",
      "6  Heterog√™neo  fold_6  0.863033  0.887991   [[383, 74], [97, 769]]  0.899941   \n",
      "7  Heterog√™neo  fold_7  0.865342  0.892610   [[383, 74], [93, 773]]  0.902510   \n",
      "8  Heterog√™neo  fold_8  0.864947  0.898383   [[380, 77], [88, 778]]  0.904126   \n",
      "9  Heterog√™neo  fold_9  0.850493  0.884393  [[374, 84], [100, 765]]  0.892649   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      34.073633        0.130495    0.141994        0.964382       0.858006   \n",
      "1      34.202858        0.134072    0.132175        0.967011       0.867825   \n",
      "2      34.258089        0.114444    0.144260        0.967271       0.855740   \n",
      "3      33.944908        0.120451    0.135299        0.966512       0.864701   \n",
      "4      34.961690        0.119511    0.131519        0.968009       0.868481   \n",
      "5      33.611226        0.110078    0.145125        0.967890       0.854875   \n",
      "6      34.340803        0.120486    0.129252        0.967568       0.870748   \n",
      "7      34.990326        0.130127    0.126228        0.966584       0.873772   \n",
      "8      35.139434        0.139929    0.124717        0.966595       0.875283   \n",
      "9      41.023438        0.485863    0.139078        0.969186       0.860922   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.896956  0.130495  \n",
      "1   0.893060  0.134072  \n",
      "2   0.891078  0.114444  \n",
      "3   0.910394  0.120451  \n",
      "4   0.903263  0.119511  \n",
      "5   0.892774  0.110078  \n",
      "6   0.912218  0.120486  \n",
      "7   0.912633  0.130127  \n",
      "8   0.909942  0.139929  \n",
      "9   0.901060  0.485863  \n",
      "Model ANNs\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0       ANNs  fold_0  0.848377  0.884527  [[372, 86], [100, 766]]  0.891735   \n",
      "1       ANNs  fold_1  0.835423  0.849885  [[376, 82], [130, 736]]  0.874109   \n",
      "2       ANNs  fold_2  0.844325  0.896074   [[363, 95], [90, 776]]  0.893495   \n",
      "3       ANNs  fold_3  0.858962  0.857968  [[393, 64], [123, 743]]  0.888225   \n",
      "4       ANNs  fold_4  0.856347  0.883372  [[379, 78], [101, 765]]  0.895260   \n",
      "5       ANNs  fold_5  0.837597  0.856813  [[374, 83], [124, 742]]  0.877587   \n",
      "6       ANNs  fold_6  0.852611  0.845266  [[393, 64], [134, 732]]  0.880866   \n",
      "7       ANNs  fold_7  0.849115  0.857968  [[384, 73], [123, 743]]  0.883472   \n",
      "8       ANNs  fold_8  0.871967  0.894919   [[388, 69], [91, 775]]  0.906433   \n",
      "9       ANNs  fold_9  0.844839  0.838150  [[390, 68], [140, 725]]  0.874548   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0     192.260852        0.028236    0.140483        0.833581       0.859517   \n",
      "1      64.978956        0.008763    0.160121        0.832018       0.839879   \n",
      "2     135.253790        0.061810    0.139728        0.833243       0.860272   \n",
      "3      78.494214        0.046603    0.141345        0.833300       0.858655   \n",
      "4     127.584972        0.101306    0.135299        0.837082       0.864701   \n",
      "5      80.868506        0.022736    0.156463        0.838910       0.843537   \n",
      "6     120.525035        0.042248    0.149660        0.837707       0.850340   \n",
      "7     222.616320        0.055776    0.148148        0.834196       0.851852   \n",
      "8      90.786816        0.050985    0.120937        0.831563       0.879063   \n",
      "9     191.903455        0.833946    0.157218        0.834721       0.842782   \n",
      "\n",
      "   precision     loacc  \n",
      "0   0.899061  0.028236  \n",
      "1   0.899756  0.008763  \n",
      "2   0.890930  0.061810  \n",
      "3   0.920694  0.046603  \n",
      "4   0.907473  0.101306  \n",
      "5   0.899394  0.022736  \n",
      "6   0.919598  0.042248  \n",
      "7   0.910539  0.055776  \n",
      "8   0.918246  0.050985  \n",
      "9   0.914250  0.833946  \n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, ensembles, use_adasyn=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
